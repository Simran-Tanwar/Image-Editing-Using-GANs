{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624f9c49-9f53-49a1-8063-e56e0b14be9b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5115cd-8223-4357-902e-3602354b08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "#import graphs\n",
    "import importlib\n",
    "import numpy as np\n",
    "import logging\n",
    "import utils.logging\n",
    "from utils import util, image\n",
    "from options.train_options import TrainOptions\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7969951-b2b6-4ec4-86b5-0aefef1f6104",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e7cb6-a7a7-4ed3-8a6f-fb47181ca583",
   "metadata": {},
   "source": [
    "## constants - not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c42453-7df8-49f3-88c6-6bc21edebf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 4\n",
    "BATCH_SIZE = 4\n",
    "# DIM_Z = 512  # used only for setting default arg value\n",
    "DIM_Z = 512  # used only for setting default arg value\n",
    "resolution = 256\n",
    "# resolution = 128\n",
    "useGPU = True\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "\n",
    "\n",
    "# Not using\n",
    "CRITIC_ITERS = 5 # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "ITERS = 200000 # How many generator iterations to train for\n",
    "\n",
    "# MODEL_ROOT = 'resources/pgan_pretrained/'\n",
    "# net_info = dict(\n",
    "#     celebahq=dict(\n",
    "#         path=MODEL_ROOT + 'karras2018iclr-celebahq-1024x1024.pkl',\n",
    "#         img_size=1024,\n",
    "#         coco_id=None,\n",
    "#         is_face=True\n",
    "#     ),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aed05e-e784-410c-bdac-ff3d988da922",
   "metadata": {},
   "source": [
    "## graph_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0807920d-4bad-410c-82d0-57f2a8b54256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphs.pggan import constants\n",
    "def z_sample(batch_size, seed=0, dim_z=constants.DIM_Z):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    # zs = rnd.randn(batch_size, dim_z)\n",
    "    zs = rnd.randn(batch_size, dim_z)\n",
    "    return zs\n",
    "\n",
    "\n",
    "def graph_input(graph, num_samples, seed=0, **kwargs):\n",
    "    ''' creates z inputs for graph '''\n",
    "    zs = z_sample(num_samples, seed, graph.dim_z)\n",
    "    return {'z': zs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f42e1ad-0c25-4b9e-94e3-ab76c703e83b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## model_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "209b3e05-00b7-4ab8-b560-b7bbe3cb8f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def init_linear(linear):\n",
    "\tinit.xavier_normal(linear.weight)\n",
    "\tlinear.bias.data.zero_()\n",
    "\n",
    "\n",
    "def init_conv(conv, glu=True):\n",
    "\tinit.kaiming_normal(conv.weight)\n",
    "\tif conv.bias is not None:\n",
    "\t\tconv.bias.data.zero_()\n",
    "\n",
    "\n",
    "class SpectralNorm:\n",
    "\tdef __init__(self, name):\n",
    "\t\tself.name = name\n",
    "\n",
    "\tdef compute_weight(self, module):\n",
    "\t\tweight = getattr(module, self.name + '_orig')\n",
    "\t\tu = getattr(module, self.name + '_u')\n",
    "\t\tsize = weight.size()\n",
    "\t\tweight_mat = weight.contiguous().view(size[0], -1)\n",
    "\t\tif weight_mat.is_cuda:\n",
    "\t\t\tu = u.cuda()\n",
    "\t\tv = weight_mat.t() @ u\n",
    "\t\tv = v / v.norm()\n",
    "\t\tu = weight_mat @ v\n",
    "\t\tu = u / u.norm()\n",
    "\t\tweight_sn = weight_mat / (u.t() @ weight_mat @ v)\n",
    "\t\tweight_sn = weight_sn.view(*size)\n",
    "\n",
    "\t\treturn weight_sn, Variable(u.data)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef apply(module, name):\n",
    "\t\tfn = SpectralNorm(name)\n",
    "\n",
    "\t\tweight = getattr(module, name)\n",
    "\t\tdel module._parameters[name]\n",
    "\t\tmodule.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "\t\tinput_size = weight.size(0)\n",
    "\t\tu = Variable(torch.randn(input_size, 1) * 0.1, requires_grad=False)\n",
    "\t\tsetattr(module, name + '_u', u)\n",
    "\t\tsetattr(module, name, fn.compute_weight(module)[0])\n",
    "\n",
    "\t\tmodule.register_forward_pre_hook(fn)\n",
    "\n",
    "\t\treturn fn\n",
    "\n",
    "\tdef __call__(self, module, input):\n",
    "\t\tweight_sn, u = self.compute_weight(module)\n",
    "\t\tsetattr(module, self.name, weight_sn)\n",
    "\t\tsetattr(module, self.name + '_u', u)\n",
    "\n",
    "\n",
    "def spectral_norm(module, name='weight'):\n",
    "\tSpectralNorm.apply(module, name)\n",
    "\n",
    "\treturn module\n",
    "\n",
    "\n",
    "class EqualLR:\n",
    "\tdef __init__(self, name):\n",
    "\t\tself.name = name\n",
    "\n",
    "\tdef compute_weight(self, module):\n",
    "\t\tweight = getattr(module, self.name + '_orig')\n",
    "\t\tfan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "\t\treturn weight * sqrt(2 / fan_in)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef apply(module, name):\n",
    "\t\tfn = EqualLR(name)\n",
    "\n",
    "\t\tweight = getattr(module, name)\n",
    "\t\tdel module._parameters[name]\n",
    "\t\tmodule.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "\t\tmodule.register_forward_pre_hook(fn)\n",
    "\n",
    "\t\treturn fn\n",
    "\n",
    "\tdef __call__(self, module, input):\n",
    "\t\tweight = self.compute_weight(module)\n",
    "\t\tsetattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "\tEqualLR.apply(module, name)\n",
    "\n",
    "\treturn module\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True)\n",
    "\t\t\t\t\t\t\t\t  + 1e-8)\n",
    "\n",
    "\n",
    "class SpectralNormConv2d(nn.Module):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tconv = nn.Conv2d(*args, **kwargs)\n",
    "\t\tinit.kaiming_normal(conv.weight)\n",
    "\t\tconv.bias.data.zero_()\n",
    "\t\tself.conv = spectral_norm(conv)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.conv(input)\n",
    "\n",
    "\n",
    "class EqualConv2d(nn.Module):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tconv = nn.Conv2d(*args, **kwargs)\n",
    "\t\tconv.weight.data.normal_()\n",
    "\t\tconv.bias.data.zero_()\n",
    "\t\tself.conv = equal_lr(conv)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.conv(input)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\tdef __init__(self, in_channel, out_channel, kernel_size,\n",
    "\t\t\t\t padding,\n",
    "\t\t\t\t kernel_size2=None, padding2=None,\n",
    "\t\t\t\t pixel_norm=True, spectral_norm=False):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tpad1 = padding\n",
    "\t\tpad2 = padding\n",
    "\t\tif padding2 is not None:\n",
    "\t\t\tpad2 = padding2\n",
    "\n",
    "\t\tkernel1 = kernel_size\n",
    "\t\tkernel2 = kernel_size\n",
    "\t\tif kernel_size2 is not None:\n",
    "\t\t\tkernel2 = kernel_size2\n",
    "\n",
    "\t\tif spectral_norm:\n",
    "\t\t\tself.conv = nn.Sequential(SpectralNormConv2d(in_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t out_channel, kernel1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding=pad1),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2),\n",
    "\t\t\t\t\t\t\t\t\t  SpectralNormConv2d(out_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t out_channel, kernel2,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding=pad2),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2))\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif pixel_norm:\n",
    "\t\t\t\tself.conv = nn.Sequential(EqualConv2d(in_channel, out_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  kernel1, padding=pad1),\n",
    "\t\t\t\t\t\t\t\t\t\t  PixelNorm(),\n",
    "\t\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2),\n",
    "\t\t\t\t\t\t\t\t\t\t  EqualConv2d(out_channel, out_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  kernel2, padding=pad2),\n",
    "\t\t\t\t\t\t\t\t\t\t  PixelNorm(),\n",
    "\t\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2))\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.conv = nn.Sequential(EqualConv2d(in_channel, out_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  kernel1, padding=pad1),\n",
    "\t\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2),\n",
    "\t\t\t\t\t\t\t\t\t\t  EqualConv2d(out_channel, out_channel,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t  kernel2, padding=pad2),\n",
    "\t\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2))\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tout = self.conv(input)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, code_dim=512 - 10, n_label=10):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.label_embed = nn.Embedding(n_label, n_label)\n",
    "\t\tself.code_norm = PixelNorm()\n",
    "\t\tself.label_embed.weight.data.normal_()\n",
    "\t\t# self.progression = nn.ModuleList([ConvBlock(512, 512, 4, 3, 3, 1),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1),\n",
    "\t\t#                                   ConvBlock(512, 256, 3, 1),\n",
    "\t\t#                                   ConvBlock(256, 128, 3, 1),])\n",
    "\n",
    "\t\t# self.to_rgb = nn.ModuleList([nn.Conv2d(512, 3, 1),\n",
    "\t\t#                              nn.Conv2d(512, 3, 1),\n",
    "\t\t#                              nn.Conv2d(512, 3, 1),\n",
    "\t\t#                              nn.Conv2d(512, 3, 1),\n",
    "\t\t#                              nn.Conv2d(256, 3, 1),\n",
    "\t\t#                              nn.Conv2d(128, 3, 1)])\n",
    "\n",
    "\t\tself.progression = nn.ModuleList([ConvBlock(512, 512, 4, 3, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 256, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(256, 128, 3, 1),  # 128\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(128, 64, 3, 1),  # 256\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(64, 32, 3, 1),  # 512\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(32, 16, 3, 1)])  # 1024\n",
    "\n",
    "\t\tself.to_rgb = nn.ModuleList([nn.Conv2d(512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(512, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(256, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(128, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(64, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(32, 3, 1),\n",
    "\t\t\t\t\t\t\t\t\t nn.Conv2d(16, 3, 1)])\n",
    "\n",
    "\tdef forward(self, input, label=None, step=6, alpha=0):\n",
    "\t\tinput = self.code_norm(input)\n",
    "\t\t# New line\n",
    "\t\tlabel = torch.zeros(input.size(0), dtype=torch.int64).cuda()\n",
    "\n",
    "\t\tlabel = self.label_embed(label)\n",
    "\t\tout = torch.cat([input, label], 1).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\t\tfor i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "\t\t\tif i > 0 and step > 0:\n",
    "\t\t\t\tupsample = F.upsample(out, scale_factor=2)\n",
    "\t\t\t\tout = conv(upsample)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tout = conv(out)\n",
    "\n",
    "\t\t\tif i == step:\n",
    "\t\t\t\tout = to_rgb(out)\n",
    "\n",
    "\t\t\t\tif i > 0 and 0 <= alpha < 1:\n",
    "\t\t\t\t\tskip_rgb = self.to_rgb[i - 1](upsample)\n",
    "\t\t\t\t\tout = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, n_label=10):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# self.progression = nn.ModuleList([ConvBlock(128, 256, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(256, 512, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(512, 512, 3, 1,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False),\n",
    "\t\t#                                   ConvBlock(513, 512, 3, 1, 4, 0,\n",
    "\t\t#                                             pixel_norm=False,\n",
    "\t\t#                                             spectral_norm=False)])\n",
    "\n",
    "\t\tself.progression = nn.ModuleList([ConvBlock(16, 32, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(32, 64, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(64, 128, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(128, 256, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(256, 512, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(512, 512, 3, 1,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False),\n",
    "\t\t\t\t\t\t\t\t\t\t  ConvBlock(513, 512, 3, 1, 4, 0,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tpixel_norm=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tspectral_norm=False)])\n",
    "\n",
    "\t\tself.from_rgb = nn.ModuleList([nn.Conv2d(3, 16, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 32, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 64, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 128, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 256, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 512, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 512, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 512, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 512, 1),\n",
    "\t\t\t\t\t\t\t\t\t   nn.Conv2d(3, 512, 1)])\n",
    "\n",
    "\t\tself.n_layer = len(self.progression)\n",
    "\n",
    "\t\tself.linear = nn.Linear(512, 1 + n_label)\n",
    "\n",
    "\tdef forward(self, input, step=6, alpha=0):\n",
    "\t\tfor i in range(step, -1, -1):\n",
    "\t\t\tindex = self.n_layer - i - 1\n",
    "\t\t\tif i == step:\n",
    "\t\t\t\tout = self.from_rgb[index](input)\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tmean_std = input.std(0).mean()\n",
    "\t\t\t\tmean_std = mean_std.expand(input.size(0), 1, 4, 4)\n",
    "\t\t\t\tout = torch.cat([out, mean_std], 1)\n",
    "\n",
    "\t\t\tout = self.progression[index](out)\n",
    "\n",
    "\t\t\tif i > 0:\n",
    "\t\t\t\tout = F.avg_pool2d(out, 2)\n",
    "\n",
    "\t\t\t\tif i == step and 0 <= alpha < 1:\n",
    "\t\t\t\t\tskip_rgb = F.avg_pool2d(input, 2)\n",
    "\t\t\t\t\tskip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
    "\t\t\t\t\tout = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "\t\tout = out.squeeze(2).squeeze(2)\n",
    "\t\t# print(input.size(), out.size(), step)\n",
    "\t\tout = self.linear(out)\n",
    "\n",
    "\t\treturn out[:, 0], out[:, 1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca58f5-7270-464b-8f77-ff3972d8e0ac",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc75a51-5dd4-4184-8096-cdc25e55cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def init_linear(linear):\n",
    "\tinit.xavier_normal(linear.weight)\n",
    "\tlinear.bias.data.zero_()\n",
    "\n",
    "\n",
    "def init_conv(conv, glu=True):\n",
    "\tinit.kaiming_normal(conv.weight)\n",
    "\tif conv.bias is not None:\n",
    "\t\tconv.bias.data.zero_()\n",
    "\n",
    "\n",
    "class EqualLR:\n",
    "\tdef __init__(self, name):\n",
    "\t\tself.name = name\n",
    "\n",
    "\tdef compute_weight(self, module):\n",
    "\t\tweight = getattr(module, self.name + '_orig')\n",
    "\t\tfan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "\t\treturn weight * sqrt(2 / fan_in)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef apply(module, name):\n",
    "\t\tfn = EqualLR(name)\n",
    "\n",
    "\t\tweight = getattr(module, name)\n",
    "\t\tdel module._parameters[name]\n",
    "\t\tmodule.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
    "\t\tmodule.register_forward_pre_hook(fn)\n",
    "\n",
    "\t\treturn fn\n",
    "\n",
    "\tdef __call__(self, module, input):\n",
    "\t\tweight = self.compute_weight(module)\n",
    "\t\tsetattr(module, self.name, weight)\n",
    "\n",
    "\n",
    "def equal_lr(module, name='weight'):\n",
    "\tEqualLR.apply(module, name)\n",
    "\n",
    "\treturn module\n",
    "\n",
    "\n",
    "class FusedUpsample(nn.Module):\n",
    "\tdef __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tweight = torch.randn(in_channel, out_channel, kernel_size, kernel_size)\n",
    "\t\tbias = torch.zeros(out_channel)\n",
    "\n",
    "\t\tfan_in = in_channel * kernel_size * kernel_size\n",
    "\t\tself.multiplier = sqrt(2 / fan_in)\n",
    "\n",
    "\t\tself.weight = nn.Parameter(weight)\n",
    "\t\tself.bias = nn.Parameter(bias)\n",
    "\n",
    "\t\tself.pad = padding\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tweight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
    "\t\tweight = (\n",
    "\t\t\t\t\t\t weight[:, :, 1:, 1:]\n",
    "\t\t\t\t\t\t + weight[:, :, :-1, 1:]\n",
    "\t\t\t\t\t\t + weight[:, :, 1:, :-1]\n",
    "\t\t\t\t\t\t + weight[:, :, :-1, :-1]\n",
    "\t\t\t\t ) / 4\n",
    "\n",
    "\t\tout = F.conv_transpose2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class FusedDownsample(nn.Module):\n",
    "\tdef __init__(self, in_channel, out_channel, kernel_size, padding=0):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tweight = torch.randn(out_channel, in_channel, kernel_size, kernel_size)\n",
    "\t\tbias = torch.zeros(out_channel)\n",
    "\n",
    "\t\tfan_in = in_channel * kernel_size * kernel_size\n",
    "\t\tself.multiplier = sqrt(2 / fan_in)\n",
    "\n",
    "\t\tself.weight = nn.Parameter(weight)\n",
    "\t\tself.bias = nn.Parameter(bias)\n",
    "\n",
    "\t\tself.pad = padding\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tweight = F.pad(self.weight * self.multiplier, [1, 1, 1, 1])\n",
    "\t\tweight = (\n",
    "\t\t\t\t\t\t weight[:, :, 1:, 1:]\n",
    "\t\t\t\t\t\t + weight[:, :, :-1, 1:]\n",
    "\t\t\t\t\t\t + weight[:, :, 1:, :-1]\n",
    "\t\t\t\t\t\t + weight[:, :, :-1, :-1]\n",
    "\t\t\t\t ) / 4\n",
    "\n",
    "\t\tout = F.conv2d(input, weight, self.bias, stride=2, padding=self.pad)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class PixelNorm(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn input / torch.sqrt(torch.mean(input ** 2, dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "\n",
    "class BlurFunctionBackward(Function):\n",
    "\t@staticmethod\n",
    "\tdef forward(ctx, grad_output, kernel, kernel_flip):\n",
    "\t\tctx.save_for_backward(kernel, kernel_flip)\n",
    "\n",
    "\t\tgrad_input = F.conv2d(\n",
    "\t\t\tgrad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\n",
    "\t\t)\n",
    "\n",
    "\t\treturn grad_input\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(ctx, gradgrad_output):\n",
    "\t\tkernel, kernel_flip = ctx.saved_tensors\n",
    "\n",
    "\t\tgrad_input = F.conv2d(\n",
    "\t\t\tgradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\n",
    "\t\t)\n",
    "\n",
    "\t\treturn grad_input, None, None\n",
    "\n",
    "\n",
    "class BlurFunction(Function):\n",
    "\t@staticmethod\n",
    "\tdef forward(ctx, input, kernel, kernel_flip):\n",
    "\t\tctx.save_for_backward(kernel, kernel_flip)\n",
    "\n",
    "\t\toutput = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef backward(ctx, grad_output):\n",
    "\t\tkernel, kernel_flip = ctx.saved_tensors\n",
    "\n",
    "\t\tgrad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\n",
    "\n",
    "\t\treturn grad_input, None, None\n",
    "\n",
    "\n",
    "blur = BlurFunction.apply\n",
    "\n",
    "\n",
    "class Blur(nn.Module):\n",
    "\tdef __init__(self, channel):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tweight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
    "\t\tweight = weight.view(1, 1, 3, 3)\n",
    "\t\tweight = weight / weight.sum()\n",
    "\t\tweight_flip = torch.flip(weight, [2, 3])\n",
    "\n",
    "\t\tself.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\n",
    "\t\tself.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn blur(input, self.weight, self.weight_flip)\n",
    "\t\t# return F.conv2d(input, self.weight, padding=1, groups=input.shape[1])\n",
    "\n",
    "\n",
    "class EqualConv2d(nn.Module):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tconv = nn.Conv2d(*args, **kwargs)\n",
    "\t\tconv.weight.data.normal_()\n",
    "\t\tconv.bias.data.zero_()\n",
    "\t\tself.conv = equal_lr(conv)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.conv(input)\n",
    "\n",
    "\n",
    "class EqualLinear(nn.Module):\n",
    "\tdef __init__(self, in_dim, out_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tlinear = nn.Linear(in_dim, out_dim)\n",
    "\t\tlinear.weight.data.normal_()\n",
    "\t\tlinear.bias.data.zero_()\n",
    "\n",
    "\t\tself.linear = equal_lr(linear)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.linear(input)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tin_channel,\n",
    "\t\t\tout_channel,\n",
    "\t\t\tkernel_size,\n",
    "\t\t\tpadding,\n",
    "\t\t\tkernel_size2=None,\n",
    "\t\t\tpadding2=None,\n",
    "\t\t\tdownsample=False,\n",
    "\t\t\tfused=False,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tpad1 = padding\n",
    "\t\tpad2 = padding\n",
    "\t\tif padding2 is not None:\n",
    "\t\t\tpad2 = padding2\n",
    "\n",
    "\t\tkernel1 = kernel_size\n",
    "\t\tkernel2 = kernel_size\n",
    "\t\tif kernel_size2 is not None:\n",
    "\t\t\tkernel2 = kernel_size2\n",
    "\n",
    "\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\tEqualConv2d(in_channel, out_channel, kernel1, padding=pad1),\n",
    "\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t)\n",
    "\n",
    "\t\tif downsample:\n",
    "\t\t\tif fused:\n",
    "\t\t\t\tself.conv2 = nn.Sequential(\n",
    "\t\t\t\t\tBlur(out_channel),\n",
    "\t\t\t\t\tFusedDownsample(out_channel, out_channel, kernel2, padding=pad2),\n",
    "\t\t\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.conv2 = nn.Sequential(\n",
    "\t\t\t\t\tBlur(out_channel),\n",
    "\t\t\t\t\tEqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "\t\t\t\t\tnn.AvgPool2d(2),\n",
    "\t\t\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tself.conv2 = nn.Sequential(\n",
    "\t\t\t\tEqualConv2d(out_channel, out_channel, kernel2, padding=pad2),\n",
    "\t\t\t\tnn.LeakyReLU(0.2),\n",
    "\t\t\t)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tout = self.conv1(input)\n",
    "\t\tout = self.conv2(out)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "\tdef __init__(self, in_channel, style_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.norm = nn.InstanceNorm2d(in_channel)\n",
    "\t\tself.style = EqualLinear(style_dim, in_channel * 2)\n",
    "\n",
    "\t\tself.style.linear.bias.data[:in_channel] = 1\n",
    "\t\tself.style.linear.bias.data[in_channel:] = 0\n",
    "\n",
    "\tdef forward(self, input, style):\n",
    "\t\tstyle = self.style(style).unsqueeze(2).unsqueeze(3)\n",
    "\t\tgamma, beta = style.chunk(2, 1)\n",
    "\n",
    "\t\tout = self.norm(input)\n",
    "\t\tout = gamma * out + beta\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "\tdef __init__(self, channel):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
    "\n",
    "\tdef forward(self, image, noise):\n",
    "\t\treturn image + self.weight * noise\n",
    "\n",
    "\n",
    "class ConstantInput(nn.Module):\n",
    "\tdef __init__(self, channel, size=4):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tbatch = input.shape[0]\n",
    "\t\tout = self.input.repeat(batch, 1, 1, 1)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class StyledConvBlock(nn.Module):\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tin_channel,\n",
    "\t\t\tout_channel,\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t\tpadding=1,\n",
    "\t\t\tstyle_dim=512,\n",
    "\t\t\tinitial=False,\n",
    "\t\t\tupsample=False,\n",
    "\t\t\tfused=False,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tif initial:\n",
    "\t\t\tself.conv1 = ConstantInput(in_channel)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tif upsample:\n",
    "\t\t\t\tif fused:\n",
    "\t\t\t\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\t\t\t\tFusedUpsample(\n",
    "\t\t\t\t\t\t\tin_channel, out_channel, kernel_size, padding=padding\n",
    "\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\tBlur(out_channel),\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.conv1 = nn.Sequential(\n",
    "\t\t\t\t\t\tnn.Upsample(scale_factor=2, mode='nearest'),\n",
    "\t\t\t\t\t\tEqualConv2d(\n",
    "\t\t\t\t\t\t\tin_channel, out_channel, kernel_size, padding=padding\n",
    "\t\t\t\t\t\t),\n",
    "\t\t\t\t\t\tBlur(out_channel),\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.conv1 = EqualConv2d(\n",
    "\t\t\t\t\tin_channel, out_channel, kernel_size, padding=padding\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\tself.noise1 = equal_lr(NoiseInjection(out_channel))\n",
    "\t\tself.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "\t\tself.lrelu1 = nn.LeakyReLU(0.2)\n",
    "\n",
    "\t\tself.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
    "\t\tself.noise2 = equal_lr(NoiseInjection(out_channel))\n",
    "\t\tself.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
    "\t\tself.lrelu2 = nn.LeakyReLU(0.2)\n",
    "\n",
    "\tdef forward(self, input, style, noise):\n",
    "\t\tout = self.conv1(input)\n",
    "\t\tout = self.noise1(out, noise)\n",
    "\t\tout = self.lrelu1(out)\n",
    "\t\tout = self.adain1(out, style)\n",
    "\n",
    "\t\tout = self.conv2(out)\n",
    "\t\tout = self.noise2(out, noise)\n",
    "\t\tout = self.lrelu2(out)\n",
    "\t\tout = self.adain2(out, style)\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "\tdef __init__(self, code_dim, fused=True):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.progression = nn.ModuleList(\n",
    "\t\t\t[\n",
    "\t\t\t\tStyledConvBlock(512, 512, 3, 1, initial=True),  # 4\n",
    "\t\t\t\tStyledConvBlock(512, 512, 3, 1, upsample=True),  # 8\n",
    "\t\t\t\tStyledConvBlock(512, 512, 3, 1, upsample=True),  # 16\n",
    "\t\t\t\tStyledConvBlock(512, 512, 3, 1, upsample=True),  # 32\n",
    "\t\t\t\tStyledConvBlock(512, 256, 3, 1, upsample=True),  # 64\n",
    "\t\t\t\tStyledConvBlock(256, 128, 3, 1, upsample=True, fused=fused),  # 128\n",
    "\t\t\t\tStyledConvBlock(128, 64, 3, 1, upsample=True, fused=fused),  # 256\n",
    "\t\t\t\tStyledConvBlock(64, 32, 3, 1, upsample=True, fused=fused),  # 512\n",
    "\t\t\t\tStyledConvBlock(32, 16, 3, 1, upsample=True, fused=fused),  # 1024\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t\tself.to_rgb = nn.ModuleList(\n",
    "\t\t\t[\n",
    "\t\t\t\tEqualConv2d(512, 3, 1),\n",
    "\t\t\t\tEqualConv2d(512, 3, 1),\n",
    "\t\t\t\tEqualConv2d(512, 3, 1),\n",
    "\t\t\t\tEqualConv2d(512, 3, 1),\n",
    "\t\t\t\tEqualConv2d(256, 3, 1),\n",
    "\t\t\t\tEqualConv2d(128, 3, 1),\n",
    "\t\t\t\tEqualConv2d(64, 3, 1),\n",
    "\t\t\t\tEqualConv2d(32, 3, 1),\n",
    "\t\t\t\tEqualConv2d(16, 3, 1),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t\t# self.blur = Blur()\n",
    "\n",
    "\tdef forward(self, style, noise, step=0, alpha=-1, mixing_range=(-1, -1)):\n",
    "\t\tout = noise[0]\n",
    "\n",
    "\t\tif len(style) < 2:\n",
    "\t\t\tinject_index = [len(self.progression) + 1]\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tinject_index = random.sample(list(range(step)), len(style) - 1)\n",
    "\n",
    "\t\tcrossover = 0\n",
    "\n",
    "\t\tfor i, (conv, to_rgb) in enumerate(zip(self.progression, self.to_rgb)):\n",
    "\t\t\tprint('mixing_range: ', mixing_range)\n",
    "\t\t\tprint('crossover: ', crossover)\n",
    "\t\t\tif mixing_range == (-1, -1):\n",
    "\t\t\t\tif crossover < len(inject_index) and i > inject_index[crossover]:\n",
    "\t\t\t\t\tcrossover = min(crossover + 1, len(style))\n",
    "\n",
    "\t\t\t\tprint('crossover2: ', crossover)\n",
    "\t\t\t\tstyle_step = style[crossover]\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tif mixing_range[0] <= i <= mixing_range[1]:\n",
    "\t\t\t\t\tstyle_step = style[1]\n",
    "\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tstyle_step = style[0]\n",
    "\n",
    "\t\t\tif i > 0 and step > 0:\n",
    "\t\t\t\tout_prev = out\n",
    "\n",
    "\t\t\tout = conv(out, style_step, noise[i])\n",
    "\n",
    "\t\t\tif i == step:\n",
    "\t\t\t\tout = to_rgb(out)\n",
    "\n",
    "\t\t\t\tif i > 0 and 0 <= alpha < 1:\n",
    "\t\t\t\t\tskip_rgb = self.to_rgb[i - 1](out_prev)\n",
    "\t\t\t\t\tskip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='nearest')\n",
    "\t\t\t\t\tout = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class StyledGenerator(nn.Module):\n",
    "\tdef __init__(self, code_dim=512, n_mlp=8):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.generator = Generator(code_dim)\n",
    "\n",
    "\t\tlayers = [PixelNorm()]\n",
    "\t\tfor i in range(n_mlp):\n",
    "\t\t\tlayers.append(EqualLinear(code_dim, code_dim))\n",
    "\t\t\tlayers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "\t\tself.style = nn.Sequential(*layers)\n",
    "\n",
    "\tdef forward(\n",
    "\t\t\tself,\n",
    "\t\t\tinput,\n",
    "\t\t\tnoise=None,\n",
    "\t\t\tstep=0,\n",
    "\t\t\talpha=-1,\n",
    "\t\t\tmean_style=None,\n",
    "\t\t\tstyle_weight=0,\n",
    "\t\t\tmixing_range=(-1, -1),\n",
    "\t):\n",
    "\t\tstyles = []\n",
    "\t\tif type(input) not in (list, tuple):\n",
    "\t\t\tinput = [input]\n",
    "\n",
    "\t\t# print('len(input): ',len(input))\n",
    "\t\tfor i in input:\n",
    "\t\t\tstyles.append(self.style(i))\n",
    "\t\t\t# print('self.style(i).shape: ',self.style(i).shape)\n",
    "\n",
    "\t\tbatch = input[0].shape[0]\n",
    "\n",
    "\t\tif noise is None:\n",
    "\t\t\tnoise = []\n",
    "\n",
    "\t\t\tfor i in range(step + 1):\n",
    "\t\t\t\tsize = 4 * 2 ** i\n",
    "\t\t\t\tnoise.append(torch.randn(batch, 1, size, size, device=input[0].device))\n",
    "\n",
    "\t\tif mean_style is not None:\n",
    "\t\t\tstyles_norm = []\n",
    "\n",
    "\t\t\tfor style in styles:\n",
    "\t\t\t\tstyles_norm.append(mean_style + style_weight * (style - mean_style))\n",
    "\n",
    "\t\t\tstyles = styles_norm\n",
    "\n",
    "\t\treturn self.generator(styles, noise, step, alpha, mixing_range=mixing_range)\n",
    "\n",
    "\tdef mean_style(self, input):\n",
    "\t\tstyle = self.style(input).mean(0, keepdim=True)\n",
    "\n",
    "\t\treturn style\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self, fused=True, from_rgb_activate=False):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.progression = nn.ModuleList(\n",
    "\t\t\t[\n",
    "\t\t\t\tConvBlock(16, 32, 3, 1, downsample=True, fused=fused),  # 512\n",
    "\t\t\t\tConvBlock(32, 64, 3, 1, downsample=True, fused=fused),  # 256\n",
    "\t\t\t\tConvBlock(64, 128, 3, 1, downsample=True, fused=fused),  # 128\n",
    "\t\t\t\tConvBlock(128, 256, 3, 1, downsample=True, fused=fused),  # 64\n",
    "\t\t\t\tConvBlock(256, 512, 3, 1, downsample=True),  # 32\n",
    "\t\t\t\tConvBlock(512, 512, 3, 1, downsample=True),  # 16\n",
    "\t\t\t\tConvBlock(512, 512, 3, 1, downsample=True),  # 8\n",
    "\t\t\t\tConvBlock(512, 512, 3, 1, downsample=True),  # 4\n",
    "\t\t\t\tConvBlock(513, 512, 3, 1, 4, 0),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t\tdef make_from_rgb(out_channel):\n",
    "\t\t\tif from_rgb_activate:\n",
    "\t\t\t\treturn nn.Sequential(EqualConv2d(3, out_channel, 1), nn.LeakyReLU(0.2))\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn EqualConv2d(3, out_channel, 1)\n",
    "\n",
    "\t\tself.from_rgb = nn.ModuleList(\n",
    "\t\t\t[\n",
    "\t\t\t\tmake_from_rgb(16),\n",
    "\t\t\t\tmake_from_rgb(32),\n",
    "\t\t\t\tmake_from_rgb(64),\n",
    "\t\t\t\tmake_from_rgb(128),\n",
    "\t\t\t\tmake_from_rgb(256),\n",
    "\t\t\t\tmake_from_rgb(512),\n",
    "\t\t\t\tmake_from_rgb(512),\n",
    "\t\t\t\tmake_from_rgb(512),\n",
    "\t\t\t\tmake_from_rgb(512),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t\t# self.blur = Blur()\n",
    "\n",
    "\t\tself.n_layer = len(self.progression)\n",
    "\n",
    "\t\tself.linear = EqualLinear(512, 1)\n",
    "\n",
    "\tdef forward(self, input, step=0, alpha=-1):\n",
    "\t\tfor i in range(step, -1, -1):\n",
    "\t\t\tindex = self.n_layer - i - 1\n",
    "\n",
    "\t\t\tif i == step:\n",
    "\t\t\t\tout = self.from_rgb[index](input)\n",
    "\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tout_std = torch.sqrt(out.var(0, unbiased=False) + 1e-8)\n",
    "\t\t\t\tmean_std = out_std.mean()\n",
    "\t\t\t\tmean_std = mean_std.expand(out.size(0), 1, 4, 4)\n",
    "\t\t\t\tout = torch.cat([out, mean_std], 1)\n",
    "\n",
    "\t\t\tout = self.progression[index](out)\n",
    "\n",
    "\t\t\tif i > 0:\n",
    "\t\t\t\tif i == step and 0 <= alpha < 1:\n",
    "\t\t\t\t\tskip_rgb = F.avg_pool2d(input, 2)\n",
    "\t\t\t\t\tskip_rgb = self.from_rgb[index + 1](skip_rgb)\n",
    "\n",
    "\t\t\t\t\tout = (1 - alpha) * skip_rgb + alpha * out\n",
    "\n",
    "\t\tout = out.squeeze(2).squeeze(2)\n",
    "\t\t# print(input.size(), out.size(), step)\n",
    "\t\tout = self.linear(out)\n",
    "\n",
    "\t\treturn out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f485db-20ef-4ded-bc1d-21e50f700a69",
   "metadata": {},
   "source": [
    "## pggan_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7503547-23e4-4ce3-a32c-6c9ba86678bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from torch import nn\n",
    "from graphs.pggan.model import *\n",
    "\n",
    "class PGGAN():\n",
    "\n",
    "\tdef __init__(self, lr, DIM=64):\n",
    "\n",
    "\t\tn_label = 1\n",
    "\t\tcode_size = 512 - n_label\n",
    "\n",
    "\t\tself.netG = Generator(code_size, n_label).cuda()\n",
    "\t\tself.netD = Discriminator(n_label).cuda()\n",
    "\t\tself.g_running = Generator(code_size, n_label).cuda()\n",
    "\t\tself.g_running.train(False)\n",
    "\n",
    "\t\t# Parallel training\n",
    "\t\t# generator = nn.DataParallel(Generator(code_size, n_label)).cuda()\n",
    "\t\t# discriminator = nn.DataParallel(Discriminator(n_label)).cuda()\n",
    "\t\t# g_running = nn.DataParallel(Generator(code_size, n_label)).cuda()\n",
    "\t\t# g_running.train(False)\n",
    "\n",
    "\t\tself.learningRate = lr\n",
    "\t\tself.optimizerD = self.getOptimizerD()\n",
    "\t\tself.optimizerG = self.getOptimizerG()\n",
    "\t\t# self.one = torch.FloatTensor([1]).cuda()\n",
    "\t\tself.one = torch.tensor(1, dtype=torch.float).cuda()\n",
    "\t\tself.mone = (self.one * -1).cuda()\n",
    "\n",
    "\n",
    "\tdef optimizeParameters(self, input_batch, inputLabels=None):\n",
    "\t\tpass\n",
    "\n",
    "\tdef getOptimizerD(self):\n",
    "\t\treturn optim.Adam(filter(lambda p: p.requires_grad, self.netD.parameters()),\n",
    "\t\t\t\t\t\t  betas=[0.5, 0.999], lr=self.learningRate)\n",
    "\n",
    "\tdef getOptimizerG(self):\n",
    "\t\treturn optim.Adam(filter(lambda p: p.requires_grad, self.netG.parameters()),\n",
    "\t\t\t\t\t\t  betas=[0.5, 0.999], lr=self.learningRate)\n",
    "\n",
    "\tdef accumulate(self, model1, model2, decay=0.999):\n",
    "\t\tpar1 = dict(model1.named_parameters())\n",
    "\t\tpar2 = dict(model2.named_parameters())\n",
    "\n",
    "\t\tfor k in par1.keys():\n",
    "\t\t\tpar1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "\tdef getSize(self):\n",
    "\t\tsize = 2**(self.config.depth + 3)\n",
    "\t\treturn (size, size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4275f12-ef3f-4759-8e40-eafcbbcd2321",
   "metadata": {},
   "source": [
    "## pggan_256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf5bbcf-2711-436b-b0b0-33965c65c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "from torch import nn\n",
    "from graphs.pggan.model_256 import *\n",
    "\n",
    "class PGGAN():\n",
    "\n",
    "\tdef __init__(self, lr, DIM=64):\n",
    "\n",
    "\t\tn_label = 1\n",
    "\t\tcode_size = 512 - n_label\n",
    "\n",
    "\t\tself.netG = Generator(code_size, n_label).cuda()\n",
    "\t\tself.netD = Discriminator(n_label).cuda()\n",
    "\t\tself.g_running = Generator(code_size, n_label).cuda()\n",
    "\t\tself.g_running.train(False)\n",
    "\n",
    "\t\tself.learningRate = lr\n",
    "\t\tself.optimizerD = self.getOptimizerD()\n",
    "\t\tself.optimizerG = self.getOptimizerG()\n",
    "\t\tself.one = torch.tensor(1, dtype=torch.float).cuda()\n",
    "\t\tself.mone = (self.one * -1).cuda()\n",
    "\n",
    "\n",
    "\tdef optimizeParameters(self, input_batch, inputLabels=None):\n",
    "\t\tpass\n",
    "\n",
    "\tdef getOptimizerD(self):\n",
    "\t\treturn optim.Adam(filter(lambda p: p.requires_grad, self.netD.parameters()),\n",
    "\t\t\t\t\t\t  betas=[0.5, 0.999], lr=self.learningRate)\n",
    "\n",
    "\tdef getOptimizerG(self):\n",
    "\t\treturn optim.Adam(filter(lambda p: p.requires_grad, self.netG.parameters()),\n",
    "\t\t\t\t\t\t  betas=[0.5, 0.999], lr=self.learningRate)\n",
    "\n",
    "\tdef accumulate(self, model1, model2, decay=0.999):\n",
    "\t\tpar1 = dict(model1.named_parameters())\n",
    "\t\tpar2 = dict(model2.named_parameters())\n",
    "\n",
    "\t\tfor k in par1.keys():\n",
    "\t\t\tpar1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "\tdef getSize(self):\n",
    "\t\tsize = 2**(self.config.depth + 3)\n",
    "\t\treturn (size, size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4a736-a27e-48df-bab1-bd44bac695b0",
   "metadata": {},
   "source": [
    "## transform_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7d0c333-e7ed-4327-a4a9-69d07f5618ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef optimizeParameters(self, feed_dict, name=None, updateGAN=True, trainEmbed=False):\\n\\t\"\"\"\\n\\tUsed to update single operation. Next one is compatable (better)\\n\\t\"\"\"\\n\\t#target = feed_dict[\\'target\\']\\n\\tmask = feed_dict[\\'mask_out\\']\\n\\tlogit = feed_dict[\\'logit\\']\\n\\tx_real = feed_dict[\\'real_target\\']\\n\\t# Train discreted W only\\n\\tif updateGAN == False and trainEmbed == True:\\n\\t\\tself.optimizers_embed[name].zero_grad()\\n\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\tself.optimizers_embed[name].step()\\n\\t\\treturn Edit_loss\\n\\n\\telif updateGAN == False and trainEmbed == False:\\n\\t\\traise(\\'ERROR\\')\\n\\n\\t## ------- updateGAN == True, (ignore trainEmbed) ------- ##\\n\\t# Update D\\n\\tfor iter_d in range(self.CRITIC_ITERS):\\n\\t\\tself.module.optimizerD.zero_grad()\\n\\t\\tlogit = self.get_logits(feed_dict, reshape=False)\\n\\t\\t#  train with real\\n\\t\\t# x_resized = x_real.view(-1, self.OUTPUT_DIM)\\n\\t\\tD_real = self.module.netD(x_real)\\n\\t\\tD_real = D_real.mean()\\n\\t\\tD_real.backward(self.module.mone, retain_graph=True)\\n\\n\\t\\tD_fake = self.module.netD(logit)\\n\\t\\tD_fake = D_fake.mean()\\n\\t\\tD_fake.backward(self.module.one, retain_graph=True)\\n\\n\\t\\t# train with gradient penalty\\n\\t\\tgp = gradient_penalty(functools.partial(self.module.netD), x_real, logit,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tgp_mode=\\'1-gp\\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tsample_mode=\\'line\\')\\n\\n\\t\\t# gradient_penalty = calc_gradient_penalty(self.module.netD, x_real.data, logit.data, self.BATCH_SIZE)\\n\\t\\tgp.backward(retain_graph=True)\\n\\n\\t\\t# Wasserstein_D = D_real - D_fake\\n\\t\\tself.module.optimizerD.step()\\n\\n\\t# Update G\\n\\tself.module.optimizerG.zero_grad()\\n\\n\\tnew_logit = self.get_logits(feed_dict)\\n\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t# G_train_loss = self.module.netD(new_logit.view(-1, self.OUTPUT_DIM)).mean()\\n\\tG_train_loss = self.module.netD(new_logit).mean()\\n\\tEdit_loss = - self.get_edit_loss(feed_dict)\\n\\tG_train_loss += Edit_loss\\n\\t# print(\\'Edit_loss1 and G loss: \\', Edit_loss, G_train_loss)\\n\\n\\tG_train_loss.backward(self.module.mone, retain_graph=True)\\n\\tself.module.optimizerG.step()\\n\\t# Update w\\n\\tif name:\\n\\t\\tif trainEmbed:\\n\\t\\t\\tself.optimizers_embed[name].zero_grad()\\n\\t\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\t\\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\t\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\t\\tself.optimizers_embed[name].step()\\n\\t\\telse:\\n\\t\\t\\tself.optimizers[name].zero_grad()\\n\\t\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\t\\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\t\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\t\\tself.optimizers[name].step()\\n\\n\\telse:\\n\\t\\tself.optimizer.zero_grad()\\n\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\tself.optimizer.step()\\n\\n\\treturn Edit_loss\\n\\ndef optimizeParametersAll(self, feed_dict):\\n\\n\\tx_real = feed_dict[\\'real_target\\']\\n\\n\\t# Update D\\n\\tfor iter_d in range(self.CRITIC_ITERS):\\n\\t\\tself.module.optimizerD.zero_grad()\\n\\t\\tlogit = self.get_logits(feed_dict, reshape=False)\\n\\t\\t#  train with real\\n\\t\\t# x_resized = x_real.view(-1, self.OUTPUT_DIM)\\n\\t\\t# D_real = self.module.netD(x_resized)\\n\\t\\tD_real = self.module.netD(x_real)\\n\\t\\tD_real = D_real.mean()\\n\\t\\tD_real.backward(self.module.mone, retain_graph=True)\\n\\n\\t\\tD_fake = self.module.netD(logit)\\n\\t\\tD_fake = D_fake.mean()\\n\\t\\tD_fake.backward(self.module.one, retain_graph=True)\\n\\n\\t\\t# train with gradient penalty\\n\\t\\t# gradient_penalty = calc_gradient_penalty(self.module.netD, x_resized.data, logit.data, self.BATCH_SIZE)\\n\\t\\tgp = gradient_penalty(functools.partial(self.module.netD), x_real, logit,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tgp_mode=\\'1-gp\\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tsample_mode=\\'line\\')\\n\\n\\t\\tgp.backward(retain_graph=True)\\n\\t\\t# Wasserstein_D = D_real - D_fake\\n\\t\\tself.module.optimizerD.step()\\n\\n\\t# Update G\\n\\tself.module.optimizerG.zero_grad()\\n\\tnew_logit = self.get_logits(feed_dict)\\n\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t# G_train_loss = self.module.netD(new_logit.view(-1, self.OUTPUT_DIM)).mean()\\n\\n\\tG_train_loss = self.module.netD(new_logit).mean()\\n\\tEdit_loss = - self.get_edit_loss(feed_dict)\\n\\tG_train_loss += Edit_loss\\n\\tG_train_loss.backward(self.module.mone, retain_graph=True)\\n\\tself.module.optimizerG.step()\\n\\n\\t# Update w\\n\\tfor name in self.optimizers.keys():\\n\\t\\tself.optimizers[name].zero_grad()\\n\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\n\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\tself.optimizers[name].step()\\n\\n\\t# else:\\n\\t# \\tself.optimizer.zero_grad()\\n\\t# \\tnew_logit = self.get_logits(feed_dict)\\n\\t# \\tfeed_dict[\\'logit\\'] = new_logit\\n\\t# \\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t# \\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\t# \\tEdit_loss.backward(retain_graph=True)\\n\\t# \\tself.optimizer.step()\\n\\n\\treturn Edit_loss\\n\\n\\ndef optimizeParametersAll(self, feed_dict, trainEmbed, updateGAN):\\n\\tif updateGAN:\\n\\t\\tprint(\\'Update GAN\\')\\n\\t\\t# target = feed_dict[\\'target\\']\\n\\t\\tmask = feed_dict[\\'mask_out\\']\\n\\t\\tlogit = feed_dict[\\'logit\\']\\n\\t\\tx_real = feed_dict[\\'real_target\\']\\n\\n\\t\\ty_real = Variable(torch.ones(logit.size()[0]).cuda())\\n\\t\\ty_fake = Variable(torch.zeros(logit.size()[0]).cuda())\\n\\n\\t\\t# Update D\\n\\t\\tself.module.optimizerD.zero_grad()\\n\\t\\tD_real_result = self.module.netD(x_real).squeeze()\\n\\t\\t# print(D_real_result)\\n\\n\\t\\tD_real_loss = self.BCE_loss_logits(D_real_result, y_real)\\n\\t\\tD_fake_result = self.module.netD(logit).squeeze()\\n\\t\\tD_fake_loss = self.BCE_loss_logits(D_fake_result, y_fake)\\n\\t\\tD_train_loss = D_real_loss + D_fake_loss\\n\\t\\t# print(\\'D_train_loss: \\', D_train_loss)\\n\\t\\tD_train_loss.backward(retain_graph=True)\\n\\t\\tself.module.optimizerD.step()\\n\\n\\t\\t# Update G\\n\\t\\tself.module.optimizerG.zero_grad()\\n\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\n\\t\\tD_fake_result = self.module.netD(new_logit).squeeze()\\n\\t\\tG_train_loss = self.BCE_loss_logits(D_fake_result, y_real)\\n\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\tG_train_loss += self.LAMBDA * Edit_loss\\n\\t\\t# print(\\'G_train_loss: \\', G_train_loss)\\n\\t\\tG_train_loss.backward(retain_graph=True)\\n\\t\\tself.module.optimizerG.step()\\n\\n\\t# Update w\\n\\tif trainEmbed:\\n\\t\\tfor name in self.optimizers_embed.keys():\\n\\t\\t\\tself.optimizers_embed[name].zero_grad()\\n\\t\\t\\tnew_logit = self.get_logits(feed_dict)\\n\\t\\t\\tfeed_dict[\\'logit\\'] = new_logit\\n\\t\\t\\tEdit_loss = self.get_edit_loss(feed_dict)\\n\\t\\t\\t# print(\\'Edit_loss2: \\', Edit_loss)\\n\\n\\t\\t\\tEdit_loss.backward(retain_graph=True)\\n\\t\\t\\tself.optimizers_embed[name].step()\\n\\telse:\\n\\t\\traise(\\'ERROR\\')\\n\\treturn Edit_loss\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "from graphs.pggan import constants, pggan_256 #pggan_128\n",
    "from utils import image\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, grad\n",
    "# from .gradient_penalty import gradient_penalty\n",
    "import functools\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class walk_embed(nn.Module):\n",
    "\tdef __init__(self, dim_z, Nsliders):\n",
    "\t\tsuper(walk_embed, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\n",
    "\t\tself.w_embed = nn.ParameterDict({\n",
    "\t\t\t'blondhair': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [6, 1, self.dim_z, Nsliders])).cuda()),\n",
    "\t\t\t'paleskin': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [6, 1, self.dim_z, Nsliders])).cuda()),\n",
    "\t\t\t'male': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [6, 1, self.dim_z, Nsliders])).cuda())})\n",
    "\n",
    "\tdef forward(self, z, name, alpha, index_):\n",
    "\t\tz_new = z  #.cpu()\n",
    "\t\tfor i in range(self.Nsliders):\n",
    "\t\t\tz_new = z_new + self.w_embed[name][index_, :, :, i]\n",
    "\t\t\t# al = torch.unsqueeze(alpha[:, i], axis=1)\n",
    "\t\t\t# z_new = z_new + al.cpu() * self.w_embed[name][index_, :, :, i]\n",
    "\t\treturn z_new\n",
    "\n",
    "# class walk_linear(nn.Module):\n",
    "# \tdef __init__(self, dim_z, Nsliders):\n",
    "# \t\tsuper(walk_linear, self).__init__()\n",
    "# \t\tself.dim_z = dim_z\n",
    "# \t\tself.Nsliders = Nsliders\n",
    "#\n",
    "# \t\tself.w_embed = nn.ParameterDict({\n",
    "# \t\t\t'blondhair': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [1, self.dim_z, Nsliders]))),\n",
    "# \t\t\t'paleskin': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [1, self.dim_z, Nsliders]))),\n",
    "# \t\t\t'male': nn.Parameter(torch.Tensor(np.random.normal(0.0, 0.02, [1, self.dim_z, Nsliders])))})\n",
    "#\n",
    "# \tdef forward(self, z, name, alpha, index_):\n",
    "# \t\tz_new = z.cpu()\n",
    "# \t\tfor i in range(self.Nsliders):\n",
    "# \t\t\tal = torch.unsqueeze(alpha[:, i], axis=1)\n",
    "# \t\t\tz_new = z_new + al.cpu() * self.w_embed[name][:, :, i]\n",
    "# \t\treturn z_new.cuda()\n",
    "# class WalkLinearZ(nn.Module):\n",
    "# \tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "# \t\tsuper(WalkLinearZ, self).__init__()\n",
    "# \t\tself.dim_z = dim_z\n",
    "# \t\tself.step = step\n",
    "# \t\tself.Nsliders = Nsliders\n",
    "# \t\tself.w = nn.Parameter(\n",
    "# \t\t\ttorch.Tensor(np.random.normal(0.0, 0.02, [len(attrList), self.dim_z])))\n",
    "#\n",
    "# \tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "# \t\tal = alpha.cuda()\n",
    "# \t\tdirection = torch.mm(al, self.w)  # B, C; C, 512\n",
    "# \t\tout = input + direction\n",
    "# \t\treturn out\n",
    "\n",
    "class WalkLinearZ(nn.Module):\n",
    "\tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "\t\tsuper(WalkLinearZ, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.step = step\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\n",
    "\t\t# Linear indenpendent\n",
    "\t\t# self.w = nn.Parameter(\n",
    "\t\t# \ttorch.Tensor(np.random.normal(0.0, 0.02, [len(attrList), self.dim_z])))\n",
    "\n",
    "\t\t# Linear dependent\n",
    "\t\tself.linear = nn.Linear(self.dim_z, self.dim_z)\n",
    "\n",
    "\tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "\t\tal = alpha.cuda()\n",
    "\t\t# Linear dependent\n",
    "\t\tout = self.linear(input)\n",
    "\t\tdirection = al * out / torch.norm(out, dim=1, keepdim=True) * 3\n",
    "\t\tout = input + direction\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class WalkLinearZ_free(nn.Module):\n",
    "\tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "\t\tsuper(WalkLinearZ_free, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.step = step\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\n",
    "\t\t# Linear indenpendent\n",
    "\t\tself.w = nn.Parameter(\n",
    "\t\t\ttorch.Tensor(np.random.normal(0.0, 0.02, [len(attrList), self.dim_z])))\n",
    "\t\t# Linear dependent\n",
    "\tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "\t\tal = alpha.cuda()\n",
    "\t\t# Linear dependent\n",
    "\t\tdirection = al * input * self.w\n",
    "\t\tout = input + direction\n",
    "\t\treturn out\n",
    "\n",
    "class WalkMlpZ(nn.Module):\n",
    "\tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "\t\tsuper(WalkMlpZ, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.step = step\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\t\t# direction = np.zeros((1, len(attrList)))\n",
    "\t\t# direction[0, 0] = 1\n",
    "\t\t# self.direction = torch.Tensor(direction).cuda()\n",
    "\t\tself.embed = nn.Linear(len(attrList), self.dim_z//2)\n",
    "\n",
    "\t\tself.linear = nn.Sequential(*[nn.Linear(self.dim_z, self.dim_z),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z, self.dim_z),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z, self.dim_z),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z, self.dim_z)])\n",
    "\n",
    "\tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "\t\t# al = torch.unsqueeze(alpha[:, 0], axis=1).cuda()  # Batch, C\n",
    "\t\tal = alpha.cuda()\t\t\t# B, C\n",
    "\t\t# out = self.embed(al)\t\t# B, dim_z\n",
    "\t\t# out2 = self.linear(torch.cat([out, input], 1))\n",
    "\t\tout2 = self.linear(input)\n",
    "\n",
    "\t\t# out2 = al * out2 / torch.norm(out2, dim=1, keepdim=True)\n",
    "\t\t# Single\n",
    "\t\tout2 = al * out2 / torch.norm(out2, dim=1, keepdim=True) * 3\n",
    "\t\tz_new = input + out2\n",
    "\t\treturn z_new\n",
    "\n",
    "class WalkMlpZ2(nn.Module):\n",
    "\tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "\t\tsuper(WalkMlpZ2, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.step = step\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\t\t# direction = np.zeros((1, len(attrList)))\n",
    "\t\t# direction[0, 0] = 1\n",
    "\t\t# self.direction = torch.Tensor(direction).cuda()\n",
    "\n",
    "\t\tself.linear = nn.Sequential(*[nn.Linear(self.dim_z, self.dim_z*2),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z*2, self.dim_z * 2),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z*2, self.dim_z)])\n",
    "\t# nn.Linear(self.dim_z2, self.dim_z * 2),\n",
    "\t# nn.LeakyReLU(0.2, True),\n",
    "\tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "\t\t# al = torch.unsqueeze(alpha[:, 0], axis=1).cuda()  # Batch, C\n",
    "\t\tal = alpha.cuda()\t\t\t# B, C\n",
    "\t\t# out = self.embed(al)\t\t# B, dim_z\n",
    "\t\t# out2 = self.linear(torch.cat([out, input], 1))\n",
    "\t\t# out2 = al * out2 / torch.norm(out2, dim=1, keepdim=True)\n",
    "\t\t# Single\n",
    "\n",
    "\t\tout2 = self.linear(input)\n",
    "\t\tout2 = al * out2 / torch.norm(out2, dim=1, keepdim=True) * 3\n",
    "\t\t# out2 = al * out2\n",
    "\t\tz_new = input + out2\n",
    "\t\treturn z_new\n",
    "\n",
    "class WalkMlpZ3(nn.Module):\n",
    "\tdef __init__(self, dim_z, step, Nsliders, attrList):\n",
    "\t\tsuper(WalkMlpZ3, self).__init__()\n",
    "\t\tself.dim_z = dim_z\n",
    "\t\tself.step = step\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\t\t# direction = np.zeros((1, len(attrList)))\n",
    "\t\t# direction[0, 0] = 1\n",
    "\t\t# self.direction = torch.Tensor(direction).cuda()\n",
    "\n",
    "\t\tself.linear = nn.Sequential(*[nn.Linear(self.dim_z, self.dim_z*2),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z*2, self.dim_z * 2),\n",
    "\t\t\t\t\t\t\t\t\t  nn.LeakyReLU(0.2, True),\n",
    "\t\t\t\t\t\t\t\t\t  nn.Linear(self.dim_z*2, self.dim_z)])\n",
    "\n",
    "\tdef forward(self, input, alpha, layers=None, name=None, index_=None):\n",
    "\t\tal = alpha.cuda()\t\t\t# B, C\n",
    "\t\tout2 = self.linear(input)\n",
    "\t\tout2 = al * out2\n",
    "\t\tz_new = input + out2\n",
    "\t\treturn z_new\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Normalization, self).__init__()\n",
    "\t\tmean = torch.tensor([0.485, 0.456, 0.406]).cuda()\n",
    "\t\tstd = torch.tensor([0.229, 0.224, 0.225]).cuda()\n",
    "\n",
    "\t\tself.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "\t\tself.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "\tdef forward(self, img):\n",
    "\t\treturn (img - self.mean) / self.std\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ContentLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, org, shifted):\n",
    "\t\tself.loss = F.mse_loss(org.detach(), shifted)\n",
    "\t\treturn self.loss\n",
    "\n",
    "\n",
    "class TransformGraph():\n",
    "\tdef __init__(self, lr, walk_type, nsliders, loss_type, eps, N_f,\n",
    "\t\t\t\t trainEmbed, attrList, attrTable, layers, pgan_opts):\n",
    "\t\tassert (loss_type in ['l2', 'lpips']), 'unimplemented loss'\n",
    "\n",
    "\t\t# module inputs\n",
    "\t\tself.lr = lr\n",
    "\t\tself.useGPU = constants.useGPU\n",
    "\t\t# self.module = self.get_pgan_module()\n",
    "\t\tself.module = self.get_pgan_module_ganzoo()\n",
    "\n",
    "\t\tself.one = torch.tensor(1, dtype=torch.float).cuda()\n",
    "\t\tself.mone = (self.one * -1).cuda()\n",
    "\t\tself.regressor, self.reg_optmizer = self.get_reg_module()\n",
    "\t\tself.vgg19 = self.get_vgg_module()\n",
    "\n",
    "\t\tself.dim_z = constants.DIM_Z\n",
    "\t\tself.Nsliders = Nsliders = nsliders\n",
    "\t\tself.img_size = constants.resolution\n",
    "\t\tself.num_channels = constants.NUM_CHANNELS\n",
    "\t\t# self.CRITIC_ITERS = CRITIC_ITERS = constants.CRITIC_ITERS\n",
    "\t\t# self.OUTPUT_DIM = constants.OUTPUT_DIM\n",
    "\t\tself.BATCH_SIZE = constants.BATCH_SIZE\n",
    "\t\tself.LAMBDA = 0.1\n",
    "\n",
    "\t\tself.BCE_loss = nn.BCELoss()\n",
    "\t\tself.BCE_loss_logits = nn.BCEWithLogitsLoss()\n",
    "\t\tself.MSE_loss = nn.MSELoss()\n",
    "\t\tself.ContentLoss = ContentLoss()\n",
    "\t\tself.trainEmbed = trainEmbed\n",
    "\n",
    "\t\t# PGAN 256 - 6/128 - 5\n",
    "\t\tself.step = 6\n",
    "\t\tself.alpha = 0\n",
    "\n",
    "\t\tif not attrTable:\n",
    "\t\t\tself.attrTable = OrderedDict({\n",
    "\t\t\t\t'daylight': 1, 'night': 2, 'sunrisesunset': 3, 'sunny': 5,\n",
    "\t\t\t\t'clouds': 6, 'fog': 7, 'snow': 9, 'warm': 10, 'cold': 11,\n",
    "\t\t\t\t'beautiful': 13, 'flowers': 14, 'spring': 15, 'summer': 16,\n",
    "\t\t\t\t'autumn': 17, 'winter': 18, 'colorful': 20, 'dark': 24,\n",
    "\t\t\t\t'bright': 25, 'rain': 29, 'boring': 37, 'lush': 39})\n",
    "\t\telse:\n",
    "\t\t\tself.attrTable = attrTable\n",
    "\t\tself.attrList = attrList\n",
    "\t\tself.attrIdx = self.get_attr_idx()\n",
    "\n",
    "\t\tself.module.netG.eval()\n",
    "\t\tself.module.netD.eval()\n",
    "\t\tself.regressor.eval()\n",
    "\t\tself.reg_criterion = nn.MSELoss().cuda()\n",
    "\n",
    "\t\tprint('LR 1e-4 for w, walk type: ', self.lr, walk_type)\n",
    "\t\t# walk pattern\n",
    "\t\tif walk_type == 'linear':\n",
    "\t\t\tprint('Linear input not free')\n",
    "\t\t\t# self.walk = WalkLinearZ(self.dim_z, self.step, Nsliders, self.attrList).cuda()\n",
    "\t\t\tself.walk = WalkLinearZ_free(self.dim_z, self.step, Nsliders, self.attrList).cuda()\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# MLP\n",
    "\t\t\tprint(\"MLP \")\n",
    "\t\t\t# self.walk = WalkMlpZ(self.dim_z, self.step, Nsliders, self.attrList).cuda()\n",
    "\t\t\t# self.walk = WalkMlpZ2(self.dim_z, self.step, Nsliders, self.attrList).cuda()\n",
    "\t\t\tself.walk = WalkMlpZ3(self.dim_z, self.step, Nsliders, self.attrList).cuda()\n",
    "\n",
    "\t\tself.optimizer = torch.optim.Adam(self.walk.parameters(), lr=self.lr, betas=(0.5, 0.99))\n",
    "\n",
    "\t\t# # set class vars\n",
    "\t\tself.Nsliders = Nsliders\n",
    "\t\tself.y = None\n",
    "\t\tself.z = None\n",
    "\t\tself.truncation = None\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t# self.alpha = alpha\n",
    "\t\t# self.target = target\n",
    "\t\t# self.mask = mask\n",
    "\t\t# self.z_new = z_new\n",
    "\t\t# self.transformed_output = transformed_output\n",
    "\t\t# self.outputs_orig = outputs_orig\n",
    "\t\t# self.loss = loss\n",
    "\t\t# self.loss_lpips = loss_lpips\n",
    "\t\t# self.loss_l2_sample = loss_l2_sample\n",
    "\t\t# self.loss_lpips_sample = loss_lpips_sample\n",
    "\t\t# self.train_step = train_step\n",
    "\t\t\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tself.walk_type = walk_type\n",
    "\t\tself.N_f = N_f  # NN num_steps\n",
    "\t\tself.eps = eps  # NN step_size\n",
    "\t\tself.BCE_loss = nn.BCELoss()\n",
    "\n",
    "\tdef get_attr_idx(self):\n",
    "\t\tidxList = []\n",
    "\t\tfor i in self.attrList:\n",
    "\t\t\tidxList.append(self.attrTable[i])\n",
    "\t\treturn idxList\n",
    "\n",
    "\tdef get_logits(self, inputs_dict, reshape=True):\n",
    "\t\t# print('check size: ', inputs_dict['z'].size())\n",
    "\n",
    "\t\toutputs_orig = self.module.netG(inputs_dict['z'])\n",
    "\t\t# if reshape == True:\n",
    "\t\t# Default: MNIST\n",
    "\t\t# outputs_orig = outputs_orig.view(-1, 1, 28, 28)\n",
    "\t\t# CelebA\n",
    "\t\t# outputs_orig = outputs_orig.view(-1, 3, constants.resolution, constants.resolution)\n",
    "\t\tdownsampled = F.upsample(outputs_orig, size=(outputs_orig.size(2) // 2, outputs_orig.size(3) // 2), mode='bilinear')\n",
    "\t\treturn downsampled#outputs_orig\n",
    "\n",
    "\t# def get_z_new(self, z, alpha):\n",
    "\t# \tif self.walk_type == 'linear' or self.walk_type == 'NNz':\n",
    "\t# \t\tz_new = z\n",
    "\t# \t\tfor i in range(self.Nsliders):\n",
    "\t# \t\t\t# TODO: PROBLEM HERE\n",
    "\t# \t\t\tal = torch.unsqueeze(torch.Tensor(alpha[:, i]), axis=1)\n",
    "\t# \t\t\tz_new = (z_new + al * self.w[:, :, i]).cuda()\n",
    "\t# \treturn z_new\n",
    "\n",
    "\tdef get_z_new_tensor(self, z, alpha, name=None, trainEmbed=False, index_=None, layers=None):\n",
    "\t\tz_new = self.walk(z.squeeze(), alpha, name=name, index_=index_)\n",
    "\t\treturn z_new\n",
    "\n",
    "\tdef get_loss(self, feed_dict):\n",
    "\t\t# L2 loss\n",
    "\t\ttarget = feed_dict['target']\n",
    "\t\tmask = feed_dict['mask_out']\n",
    "\t\tlogit = feed_dict['logit']\n",
    "\t\tdiff = (logit - target) * mask\n",
    "\t\treturn torch.sum(diff * diff) / torch.sum(mask)\n",
    "\n",
    "\tdef get_edit_loss(self, feed_dict):\n",
    "\t\t# L2 loss\n",
    "\t\ttarget = feed_dict['target']\n",
    "\t\tmask = feed_dict['mask_out']\n",
    "\t\tlogit = feed_dict['logit']\n",
    "\t\tdiff = (logit - target) * mask\n",
    "\t\treturn torch.sum(diff.pow(2)) / torch.sum(mask)\n",
    "\n",
    "\tdef get_reg_preds(self, logit):\n",
    "\t\tpreds = self.regressor(logit)[:, self.attrIdx]\n",
    "\t\tif len(preds.size()) == 1:\n",
    "\t\t\tpreds = preds.unsqueeze(1)\n",
    "\t\treturn preds\n",
    "\n",
    "\tdef get_alphas(self, alpha_org, alpha_delta):\n",
    "\t\t# [N, C]\n",
    "\t\talpha_target = torch.clamp(alpha_org + alpha_delta, min=0, max=1)\n",
    "\t\talpha_delta_new = alpha_target - alpha_org\n",
    "\t\t# alpha_delta_new N, C\n",
    "\t\t# alpha_target N, C\n",
    "\t\treturn alpha_target, alpha_delta_new\n",
    "\n",
    "\tdef get_bce_loss(self, pred, y, eps=1e-12):\n",
    "\t\tloss = -(y * pred.clamp(min=eps).log() + (1 - y) * (1 - pred).clamp(min=eps).log()).mean()\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef get_reg_loss(self, feed_dict):\n",
    "\t\tlogit = feed_dict['logit']\n",
    "\t\talpha_gt = feed_dict['alpha'].to(torch.double)\n",
    "\t\t# preds = self.regressor(logit)[:, self.attrTable[self.attrList[0]]]\n",
    "\t\tpreds = self.regressor(logit)[:, self.attrIdx]\n",
    "\t\tpreds = preds.unsqueeze(1).to(torch.double)\n",
    "\t\t# BCE\n",
    "\t\tloss = self.get_bce_loss(preds, alpha_gt)\n",
    "\t\t# MSE\n",
    "\t\t# loss = self.reg_criterion(preds, alpha_gt)\n",
    "\t\treturn loss.mean()\n",
    "\n",
    "\tdef get_content_loss(self, org_img, shifted_img):\n",
    "\t\tcontent_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4']\n",
    "\t\tnorm = Normalization().cuda()\n",
    "\t\tmodel = nn.Sequential(norm)\n",
    "\t\t# model = nn.Sequential()\n",
    "\n",
    "\t\ti = 0\n",
    "\t\tcontent_losses = []\n",
    "\t\tfor layer in self.vgg19.children():\n",
    "\t\t\tif isinstance(layer, nn.Conv2d):\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\tname = 'conv_{}'.format(i)\n",
    "\t\t\telif isinstance(layer, nn.ReLU):\n",
    "\t\t\t\tname = 'relu_{}'.format(i)\n",
    "\t\t\t\tlayer = nn.ReLU(inplace=False)\n",
    "\t\t\telif isinstance(layer, nn.MaxPool2d):\n",
    "\t\t\t\tname = 'pool_{}'.format(i)\n",
    "\t\t\telif isinstance(layer, nn.BatchNorm2d):\n",
    "\t\t\t\tname = 'bn_{}'.format(i)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise RuntimeError('Unrecognized layer: {}'\n",
    "\t\t\t\t\t\t\t\t   .format(layer.__class__.__name__))\n",
    "\t\t\tmodel.add_module(name, layer)\n",
    "\t\t\tif name in content_layers:\n",
    "\t\t\t\torg_content = model(org_img).detach()\n",
    "\t\t\t\tshifted_content = model(shifted_img)\n",
    "\t\t\t\tcontent_loss = self.ContentLoss(org_content, shifted_content)\n",
    "\t\t\t\t# self.loss = F.mse_loss(org.detach(), shifted)\n",
    "\t\t\t\tcontent_losses.append(content_loss)\n",
    "\t\treturn content_losses\n",
    "\n",
    "\tdef optimizeParametersAll(self, feed_dict, trainEmbed, updateGAN,\n",
    "\t\t\t\t\t\t\t  no_content_loss=False, no_gan_loss=False):\n",
    "\t\t# FOR loaded PGAN\n",
    "\t\t# if updateGAN:\n",
    "\t\t# \tprint('Update GAN')\n",
    "\t\t# \t# target = feed_dict['target']\n",
    "\t\t# \tmask = feed_dict['mask_out']\n",
    "\t\t# \tlogit = feed_dict['logit']\n",
    "\t\t# \tx_real = feed_dict['real_target']\n",
    "\t\t#\n",
    "\t\t# \ty_real = Variable(torch.ones(logit.size()[0]).cuda())\n",
    "\t\t# \ty_fake = Variable(torch.zeros(logit.size()[0]).cuda())\n",
    "\t\t#\n",
    "\t\t# \t# Update D\n",
    "\t\t# \tself.module.optimizerD.zero_grad()\n",
    "\t\t#\n",
    "\t\t# \t# D real\n",
    "\t\t# \tD_real_result = self.module.netD(x_real).squeeze()\n",
    "\t\t# \tD_real_result = D_real_result.mean() - 0.001 * (D_real_result ** 2).mean()\n",
    "\t\t# \tD_real_result.backward(self.module.mone, retain_graph=True)\n",
    "\t\t#\n",
    "\t\t# \t# D fake\n",
    "\t\t# \tD_fake_result = self.module.netD(logit).squeeze()\n",
    "\t\t# \tD_fake_result = D_fake_result.mean()\n",
    "\t\t# \tD_fake_result.backward(self.module.one, retain_graph=True)\n",
    "\t\t#\n",
    "\t\t# \t# TRAIN WITH GRADIENT PENALTY\n",
    "\t\t# \t# gp = gradient_penalty(functools.partial(self.module.netD), x_real, logit,\n",
    "\t\t# \t# \t\t\t\t\t  gp_mode='1-gp',\n",
    "\t\t# \t# \t\t\t\t\t  sample_mode='line')\n",
    "\t\t# \t# gradient_penalty = calc_gradient_penalty(self.module.netD, x_real.data, logit.data, self.BATCH_SIZE)\n",
    "\t\t#\n",
    "\t\t# \teps = torch.rand(constants.BATCH_SIZE, 1, 1, 1).cuda()\n",
    "\t\t# \tx_hat = eps * x_real.data + (1 - eps) * logit.data\n",
    "\t\t# \tx_hat = Variable(x_hat, requires_grad=True)\n",
    "\t\t# \that_predict = self.module.netD(x_hat)\n",
    "\t\t# \tgrad_x_hat = grad(outputs=hat_predict.sum(), inputs=x_hat, create_graph=True)[0]\n",
    "\t\t# \tgrad_penalty = ((grad_x_hat.view(grad_x_hat.size(0), -1).norm(2, dim=1) - 1) ** 2).mean()\n",
    "\t\t# \tgrad_penalty = 10 * grad_penalty\n",
    "\t\t# \tgrad_penalty.backward(retain_graph=True)\n",
    "\t\t#\n",
    "\t\t# \t# grad_loss_val = grad_penalty.data\n",
    "\t\t# \t# disc_loss_val = (real_predict - fake_predict).data\n",
    "\t\t# \tself.module.optimizerD.step()\n",
    "\t\t#\n",
    "\t\t# \t# Update G\n",
    "\t\t# \tself.module.optimizerG.zero_grad()\n",
    "\t\t# \tnew_logit = self.get_logits(feed_dict)\n",
    "\t\t# \tfeed_dict['logit'] = new_logit\n",
    "\t\t#\n",
    "\t\t# \tD_fake_result = self.module.netD(new_logit).squeeze()\n",
    "\t\t# \tG_train_loss = self.BCE_loss_logits(D_fake_result, y_real)\n",
    "\t\t#\n",
    "\t\t# \tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t# \tG_train_loss += self.LAMBDA * Edit_loss\n",
    "\t\t# \tG_train_loss.backward(retain_graph=True)\n",
    "\t\t# \tself.module.optimizerG.step()\n",
    "\t\t# \tself.module.accumulate(self.module.g_running, self.module.netG)\n",
    "\n",
    "\t\t# Update w\n",
    "\t\t#if trainEmbed:\n",
    "\t\t#\tprint('Update w_embed')\n",
    "\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\t# D loss\n",
    "\t\tlogit = feed_dict['logit']\n",
    "\t\t# D_fake_result, _ = self.module.netD(logit)  # B, 1\n",
    "\t\tD_fake_result = self.module.netD(F.upsample(logit, size=(int(logit.size(2)*2), int(logit.size(3)*2)), mode='bilinear'))  # B, 1\n",
    "\n",
    "\t\ty_real = Variable(torch.ones_like(D_fake_result).cuda())\n",
    "\t\tgan_loss = self.BCE_loss_logits(D_fake_result, y_real)\n",
    "\n",
    "\t\t# Content loss\n",
    "\t\tcontent_loss_list = self.get_content_loss(feed_dict['org'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  feed_dict['logit'])\n",
    "\t\tcontent_losses = 0\n",
    "\t\tfor i in range(len(content_loss_list)):\n",
    "\t\t\tcontent_losses += content_loss_list[i]\n",
    "\t\tcontent_losses = content_losses / len(content_loss_list)\n",
    "\n",
    "\t\t# Regression loss\n",
    "\t\treg_loss = self.get_reg_loss(feed_dict)\n",
    "\n",
    "\t\tif no_content_loss or  no_gan_loss:\n",
    "\t\t\tloss = reg_loss\n",
    "\t\telse:\n",
    "\t\t\tloss = 10 * reg_loss\n",
    "\n",
    "\t\tif not no_content_loss:\n",
    "\t\t\tloss += 0.05 * content_losses  # 0.05 * content_losses\n",
    "\t\tif not no_gan_loss:\n",
    "\t\t\tloss += 0.05 * gan_loss\n",
    "\t\tloss.backward()\n",
    "\t\t# for name, parms in self.walk.linear.named_parameters():\n",
    "\t\t# \tprint('-->name:', name, '-->grad_requirs:', parms.requires_grad, \\\n",
    "\t\t# \t\t  ' -->grad_value:', parms.grad)\n",
    "\t\tself.optimizer.step()\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef save_multi_models(self, save_path_w, save_path_gan,\n",
    "\t\t\t\t\t\t  trainEmbed=False, updateGAN=False,\n",
    "\t\t\t\t\t\t  single_transform_name=None):\n",
    "\t\tprint('Save W and GAN in %s and %s' % (save_path_w, save_path_gan))\n",
    "\t\tif updateGAN == True:\n",
    "\t\t\tprint('Save GAN')\n",
    "\t\t\ttorch.save(self.module, save_path_gan)\n",
    "\t\ttorch.save(self.walk, save_path_w + '_walk_module.ckpt')\n",
    "\n",
    "\tdef load_multi_models(self, save_path_w, save_path_gan,\n",
    "\t\t\t\t\t\t  trainEmbed=False, updateGAN=False,\n",
    "\t\t\t\t\t\t  single_transform_name=None):\n",
    "\t\tif updateGAN:\n",
    "\t\t\tprint('Load GAN in %s' % save_path_gan)\n",
    "\t\t\tself.module = torch.load(save_path_gan)\n",
    "\n",
    "\t\tprint('Load w in %s' % save_path_w)\n",
    "\t\tself.walk = torch.load(save_path_w)\n",
    "\n",
    "\tdef get_reg_module(self):\n",
    "\t\t#####\n",
    "\t\tmodel = torch.hub.load('pytorch/vision:v0.5.0', 'resnet50', pretrained=False)\n",
    "\t\tmodel.fc = torch.nn.Linear(2048, 40)\n",
    "\t\tmodel = model.cuda()\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\t\t#####\n",
    "\t\t# base_dir = '/home/peiye/ImageEditing/scene_regressor/checkpoint_256/'\n",
    "\t\t# ckpt = torch.load(base_dir + '500_dict.model')\n",
    "\n",
    "\t\tbase_dir = '/shared/rsaas/zpy/celeba_reg/checkpoint/'\n",
    "\t\tckpt = torch.load(base_dir + '108_dict.model')\n",
    "\t\tmodel.load_state_dict(ckpt['model'])\n",
    "\t\toptimizer.load_state_dict(ckpt['optm'])\n",
    "\t\treturn model, optimizer\n",
    "\n",
    "\n",
    "\tdef get_vgg_module(self):\n",
    "\t\t#####\n",
    "\t\timport torchvision.models as models\n",
    "\t\tvgg19 = models.vgg19(pretrained=True).features.cuda().eval()\n",
    "\t\treturn vgg19\n",
    "\n",
    "\tdef get_pgan_module_ganzoo(self):\n",
    "\t\tprint('Loading PGGAN module from ganzoo')\n",
    "\t\t# 256\n",
    "\t\t# module = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN',\n",
    "\t\t# \t\t\t\t\t\tpretrained=True,\n",
    "\t\t# \t\t\t\t\t\tmodel_name='celebAHQ-256',\n",
    "\t\t# \t\t\t\t\t\tuseGPU=self.useGPU)\n",
    "\n",
    "\t\t# 512\n",
    "\t\tmodule = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub',\n",
    "\t\t\t\t\t\t\t   'PGAN', model_name='celebAHQ-512',\n",
    "\t\t\t\t\t\t\t   pretrained=True, useGPU=True)\n",
    "\t\treturn module\n",
    "\n",
    "\tdef get_pgan_module(self):\n",
    "\t\tprint('Loading PGGAN module')\n",
    "\t\t# generator = model.Generator(511, 1).cuda().train(False)\n",
    "\t\t# module = pggan_128.PGGAN(lr=self.lr)\n",
    "\t\t# load_pretrain\n",
    "\t\t# base_dir = '/home/peiye/ImageEditing/progressive-gan-pytorch/checkpoint/'\n",
    "\t\t# ckpt = torch.load(base_dir + '510000_dict.model')\n",
    "\t\t# base_dir = '/home/peiye/ImageEditing/vision_16_pgan/checkpoint/'\n",
    "\n",
    "\t\tmodule = pggan_256.PGGAN(lr=self.lr)\n",
    "\n",
    "\t\tbase_dir = '/home/peiye/ImageEditing/pgan_scene/pgan256/checkpoint/'\n",
    "\t\t# ckpt = torch.load(base_dir + '550000_dict.model')\n",
    "\t\tckpt = torch.load(base_dir + '280000_dict.model')\n",
    "\t\tprint('Start loading PGGAN_scene 256 module in %s' % (base_dir))\n",
    "\n",
    "\t\tfrom collections import OrderedDict\n",
    "\t\tnew_state_dict = OrderedDict()\n",
    "\t\tfor k, v in ckpt['G'].items():\n",
    "\t\t\tname = k[7:]\n",
    "\t\t\tnew_state_dict[name] = v\n",
    "\t\tmodule.netG.load_state_dict(new_state_dict)\n",
    "\n",
    "\t\tnew_state_dict = OrderedDict()\n",
    "\t\tfor k, v in ckpt['D'].items():\n",
    "\t\t\tname = k[7:]\n",
    "\t\t\tnew_state_dict[name] = v\n",
    "\t\tmodule.netD.load_state_dict(new_state_dict)\n",
    "\t\tprint('Finish loading the pretrained model')\n",
    "\t\treturn module\n",
    "\n",
    "\tdef clip_ims(self, ims):\n",
    "\t\treturn np.uint8(np.clip(((ims + 1) / 2.0) * 256, 0, 255))\n",
    "\n",
    "\t# FOR WGANGP\n",
    "\t# def clip_ims(self, ims):\n",
    "\t# \treturn np.uint8(np.clip(ims * 255, 0, 255))\n",
    "\n",
    "\tdef apply_alpha(self, graph_inputs, alpha_to_graph,\n",
    "\t\t\t\t\tlayers=None, name=None,\n",
    "\t\t\t\t\ttrainEmbed=False, index_=None,\n",
    "\t\t\t\t\tgiven_w=None):\n",
    "\t\tzs_batch = graph_inputs['z']  # tensor.cuda() # [Batch, DIM_Z]\n",
    "\t\tfeed_dict = {'z': zs_batch}\n",
    "\t\tout_zs = self.get_logits(feed_dict)\n",
    "\t\talpha_to_graph = torch.tensor(alpha_to_graph).float().cuda()\n",
    "\t\talpha_org = self.get_reg_preds(out_zs)  # [B, C]\n",
    "\t\talpha_delta = torch.zeros_like(alpha_org).cuda()  # alpha_org - alpha_org\n",
    "\t\t######\n",
    "\t\t# BUG:\n",
    "\t\t# alpha_delta\n",
    "\t\talpha_delta = alpha_to_graph - alpha_org\n",
    "\n",
    "\t\t# if index_ != None:\n",
    "\t\t# \t# alpha_to_graph [B, 1]\n",
    "\t\t# \t# alpha_delta [B, attrList]\n",
    "\t\t# \t# for j in range(len(self.attrIdx)):\n",
    "\t\t# \tif len(self.attrIdx) == len(self.attrTable):\n",
    "\t\t# \t\talpha_delta[:, index_] = torch.Tensor(alpha_to_graph[:, 0]).cuda() - alpha_org[:, index_]\n",
    "\t\t# \t# print('alpha_delta[:, index_]: ', alpha_delta[:, index_])\n",
    "\t\t# \telse:\n",
    "\t\t# \t\ti = self.attrIdx.index(index_)\n",
    "\t\t# \t\t# print('Here: ', index_, i, alpha_to_graph.shape, alpha_org.size())\n",
    "\t\t# \t\talpha_delta[:, i] = torch.Tensor(alpha_to_graph[:, 0]).cuda() - alpha_org[:, i]\n",
    "\t\t#\n",
    "\t\t# print('alpha_to_graph.size(): ', alpha_to_graph.ssize(), alpha_org.size())\n",
    "\t\t# print('alpha_delta: ', alpha_delta,  alpha_delta.size(), self.attrIdx)\n",
    "\n",
    "\t\tz_new = self.get_z_new_tensor(zs_batch, alpha_delta, name, trainEmbed=trainEmbed, index_=index_)\n",
    "\t\tbest_inputs = {'z': z_new}\n",
    "\n",
    "\t\tbest_im_out = self.get_logits(best_inputs)\n",
    "\t\treturn best_im_out, alpha_org\n",
    "\n",
    "\tdef L2_loss(self, img1, img2, mask):\n",
    "\t\treturn np.sum(np.square((img1 - img2) * mask), (1, 2, 3))\n",
    "\n",
    "\tdef vis_image_batch_alphas(self, graph_inputs, filename,\n",
    "\t\t\t\t\t\t\t   alphas_to_graph, alphas_to_target,\n",
    "\t\t\t\t\t\t\t   batch_start, name=None, wgt=False, wmask=False,\n",
    "\t\t\t\t\t\t\t   trainEmbed=False, computeL2=True):\n",
    "\n",
    "\t\tzs_batch = graph_inputs['z']  # numpy\n",
    "\t\tfilename_base = filename\n",
    "\t\tims_target = []\n",
    "\t\tims_transformed = []\n",
    "\t\tims_mask = []\n",
    "\n",
    "\t\t# index_ = [i in range(len(alphas_to_graph))]\n",
    "\t\tL2_loss = {}\n",
    "\t\tprint('len(alphas_to_graph)', len(alphas_to_graph))\n",
    "\n",
    "\t\tfor index_, (ag, at) in enumerator(zip(alphas_to_graph, alphas_to_target)):\n",
    "\t\t\tprint('Index: ', index)\n",
    "\n",
    "\t\t\tinput_test = {'z': torch.Tensor(zs_batch).cuda()}\n",
    "\t\t\tout_input_test = self.get_logits(input_test)\n",
    "\t\t\tout_input_test = out_input_test.detach().cpu().numpy()  # on Cuda\n",
    "\t\t\ttarget_fn, mask_out = self.get_target_np(out_input_test, at)\n",
    "\n",
    "\t\t\tbest_im_out = self.apply_alpha(input_test, ag, name, index_).detach().cpu().numpy()\n",
    "\n",
    "\t\t\tL2_loss[at] = self.L2_loss(target_fn, best_im_out, mask_out)\n",
    "\t\t\tims_target.append(target_fn)\n",
    "\t\t\tims_transformed.append(best_im_out)\n",
    "\t\t\tims_mask.append(mask_out)\n",
    "\t\tif computeL2:\n",
    "\t\t\t## Compute L2 loss for drawing the plots\n",
    "\t\t\treturn L2_loss\n",
    "\n",
    "\t\t######### ######### ######### ######### #########\n",
    "\t\tprint('wgt: ', wgt)\n",
    "\t\tfor ii in range(zs_batch.shape[0]):\n",
    "\t\t\tarr_gt = np.stack([x[ii, :, :, :] for x in ims_target], axis=0)\n",
    "\n",
    "\t\t\tif wmask:\n",
    "\t\t\t\tarr_transform = np.stack([x[j, :, :, :] * y[j, :, :, :] for x, y\n",
    "\t\t\t\t\t\t\t\t\t\t  in zip(ims_transformed, ims_mask)], axis=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tarr_transform = np.stack([x[ii, :, :, :] for x in\n",
    "\t\t\t\t\t\t\t\t\t\t  ims_transformed], axis=0)\n",
    "\t\t\tarr_gt = self.clip_ims(arr_gt)\n",
    "\t\t\tarr_transform = self.clip_ims(arr_transform)\n",
    "\t\t\tif wgt:\n",
    "\t\t\t\tims = np.concatenate((arr_gt, arr_transform), axis=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tims = arr_transform\n",
    "\t\t\tfilename = filename_base + '_sample{}'.format(ii + batch_start)\n",
    "\t\t\tif wgt:\n",
    "\t\t\t\tfilename += '_wgt'\n",
    "\t\t\tif wmask:\n",
    "\t\t\t\tfilename += '_wmask'\n",
    "\t\t\t# (7, 3, 64, 64)\n",
    "\t\t\tif ims.shape[1] == self.num_channels:\n",
    "\t\t\t\t# N C W H -> N W H C\n",
    "\t\t\t\tims = np.transpose(ims, [0, 2, 3, 1])\n",
    "\t\t\t# ims = np.squeeze(ims)\n",
    "\t\t\t# print('ims.shape: ', ims.shape)\n",
    "\t\t\timage.save_im(image.imgrid(ims, cols=len(alphas_to_graph)), filename)\n",
    "\n",
    "\tdef vis_multi_image_batch_alphas(self, graph_inputs, filename,\n",
    "\t\t\t\t\t\t\t\t\t alphas_to_graph, alphas_to_target,\n",
    "\t\t\t\t\t\t\t\t\t batch_start,\n",
    "\t\t\t\t\t\t\t\t\t layers=None,\n",
    "\t\t\t\t\t\t\t\t\t name=None, wgt=False, wmask=False,\n",
    "\t\t\t\t\t\t\t\t\t trainEmbed=False, computeL2=False,\n",
    "\t\t\t\t\t\t\t\t\t given_w=None, index_=None):\n",
    "\t\t# TODO:\n",
    "\t\t# CHANGE!!\n",
    "\t\tzs_batch = graph_inputs['z']  # numpy\n",
    "\n",
    "\t\tfilename_base = filename\n",
    "\t\tims_target = []\n",
    "\t\tims_transformed = []\n",
    "\t\tims_mask = []\n",
    "\t\tL2_loss = {}\n",
    "\t\tindex_ = 0\n",
    "\t\tfor ag, at in zip(alphas_to_graph, alphas_to_target):\n",
    "\t\t\tinput_test = {'z': torch.Tensor(zs_batch).cuda()}\n",
    "\n",
    "\t\t\tbest_im_out, alpha_org = self.apply_alpha(input_test, ag, name=name, layers=layers,\n",
    "\t\t\t\t\t\t\t\t\t\t   trainEmbed=trainEmbed, given_w=given_w, index_=index_)\n",
    "\t\t\t# best_im_out = F.interpolate(best_im_out, size=256)\n",
    "\t\t\tbest_im_out = best_im_out.detach().cpu().numpy()\n",
    "\t\t\tbest_im_out = np.uint8(np.clip(((best_im_out + 1) / 2.0) * 255, 0, 255))\n",
    "\t\t\tims_transformed.append(best_im_out)\n",
    "\n",
    "\n",
    "\t\tfor ii in range(zs_batch.shape[0]):\n",
    "\t\t\tif wmask:\n",
    "\t\t\t\tarr_transform = np.stack([x[j, :, :, :] * y[j, :, :, :] for x, y\n",
    "\t\t\t\t\t\t\t\t\t\t  in zip(ims_transformed, ims_mask)], axis=0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tarr_transform = np.stack([x[ii, :, :, :] for x in\n",
    "\t\t\t\t\t\t\t\t\t\t  ims_transformed], axis=0)\n",
    "\t\t\tims = arr_transform\n",
    "\t\t\tfilename = filename_base + '_sample{}'.format(ii + batch_start)\n",
    "\t\t\tif wgt:\n",
    "\t\t\t\tfilename += '_wgt'\n",
    "\t\t\tif wmask:\n",
    "\t\t\t\tfilename += '_wmask'\n",
    "\t\t\t# (7, 3, 64, 64)\n",
    "\t\t\tif ims.shape[1] == 1 or ims.shape[1] == 3:\n",
    "\t\t\t\t# N C W H -> N W H C\n",
    "\t\t\t\tims = np.transpose(ims, [0, 2, 3, 1])\n",
    "\t\t\t# ims = np.squeeze(ims)\n",
    "\n",
    "\t\t\ta_org = alpha_org[ii, ]\n",
    "\n",
    "\t\t\timage.save_im(image.imgrid(ims, cols=len(alphas_to_graph)), filename)\n",
    "\n",
    "\tdef vis_image_batch(self, graph_inputs, filename,\n",
    "\t\t\t\t\t\tbatch_start, wgt=False, wmask=False, num_panels=7):\n",
    "\t\traise NotImplementedError('Subclass should implement vis_image_batch')\n",
    "\n",
    "\n",
    "class BboxTransform(TransformGraph):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tTransformGraph.__init__(self, *args, **kwargs)\n",
    "\n",
    "\tdef get_distribution_statistic(self, img, channel=None):\n",
    "\t\traise NotImplementedError('Subclass should implement get_distribution_statistic')\n",
    "\n",
    "\t# def get_category_list(self):\n",
    "\t#\treturn get_coco_imagenet_categories()\n",
    "\n",
    "\tdef distribution_data_per_category(self, num_categories, num_samples,\n",
    "\t\t\t\t\t\t\t\t\t   output_path, channel=None):\n",
    "\t\traise NotImplementedError('Coming soon')\n",
    "\n",
    "\tdef distribution_model_per_category(self, num_categories, num_samples,\n",
    "\t\t\t\t\t\t\t\t\t\ta, output_path, channel=None):\n",
    "\t\traise NotImplementedError('Coming soon')\n",
    "\n",
    "\tdef get_distributions_per_category(self, num_categories, num_samples,\n",
    "\t\t\t\t\t\t\t\t\t   output_path, palpha, nalpha,\n",
    "\t\t\t\t\t\t\t\t\t   channel=None):\n",
    "\t\traise NotImplementedError('Coming soon')\n",
    "\n",
    "\tdef get_distributions_all_categories(self, num_samples, output_path,\n",
    "\t\t\t\t\t\t\t\t\t\t channel=None):\n",
    "\t\traise NotImplementedError('Coming soon')\n",
    "\n",
    "\n",
    "class PixelTransform(TransformGraph):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tTransformGraph.__init__(self, *args, **kwargs)\n",
    "\n",
    "\tdef get_distribution_statistic(self, img, channel=None):\n",
    "\t\traise NotImplementedError('Subclass should implement get_distribution_statistic')\n",
    "\n",
    "\tdef get_distribution(self, num_samples, channel=None):\n",
    "\t\trandom_seed = 0\n",
    "\t\trnd = np.random.RandomState(random_seed)\n",
    "\t\tinputs = graph_input(self, num_samples, seed=random_seed)\n",
    "\t\tbatch_size = constants.BATCH_SIZE\n",
    "\t\tmodel_samples = []\n",
    "\t\tfor a in self.test_alphas():\n",
    "\t\t\tdistribution = []\n",
    "\t\t\tstart = time.time()\n",
    "\t\t\tprint(\"Computing attribute statistic for alpha={:0.2f}\".format(a))\n",
    "\t\t\tfor batch_num, batch_start in enumerate(range(0, num_samples,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t  batch_size)):\n",
    "\t\t\t\ts = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "\t\t\t\tinputs_batch = util.batch_input(inputs, s)\n",
    "\t\t\t\tzs_batch = inputs_batch[self.z]\n",
    "\t\t\t\ta_graph = self.scale_test_alpha_for_graph(a, zs_batch, channel)\n",
    "\t\t\t\tims = self.clip_ims(self.apply_alpha(inputs_batch, a_graph))\n",
    "\t\t\t\tfor img in ims:\n",
    "\t\t\t\t\timg_stat = self.get_distribution_statistic(img, channel)\n",
    "\t\t\t\t\tdistribution.extend(img_stat)\n",
    "\t\t\tend = time.time()\n",
    "\t\t\tprint(\"Sampled {} images in {:0.2f} min\".format(num_samples, (end - start) / 60))\n",
    "\t\t\tmodel_samples.append(distribution)\n",
    "\n",
    "\t\tmodel_samples = np.array(model_samples)\n",
    "\t\treturn model_samples\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\tdef load_multi_models(self, save_path_w, save_path_gan, trainEmbed=False, updateGAN=False,\n",
    "\t\t\t\t\t\t  single_transform_name=None):\n",
    "\t\t# Load GAN\n",
    "\t\tprint('Load GAN in %s' % save_path_gan)\n",
    "\t\t# self.module.load(save_path_gan)\n",
    "\t\tif updateGAN:\n",
    "\t\t\tprint('Load GAN')\n",
    "\t\t\tself.module = torch.load(save_path_gan)\n",
    "\n",
    "\t\tprint('Load w in %s' % save_path_w)\n",
    "\t\ttry:\n",
    "\t\t\tself.walk = torch.load(save_path_w)\n",
    "\t\texcept:\n",
    "\t\t\tfor name in self.walk.w_embed.keys():\n",
    "\t\t\t\tnew_w_path = save_path_w + '_' + name + '.npy'\n",
    "\t\t\t\tprint('Load W of %s' % name)\n",
    "\t\t\t\tprint('Before w: ', self.walk.w_embed[name].size())\n",
    "\t\t\t\tself.walk.w_embed[name] = torch.nn.Parameter(torch.Tensor(np.load(new_w_path)).cuda())\n",
    "\t\t\t\tprint('After w: ', self.walk.w_embed[name].size())\n",
    "\t\t# if trainEmbed:\n",
    "\t\t# \t# Load w\n",
    "\t\t# \tif single_transform_name:\n",
    "\t\t# \t\tprint('Load %s only ' % single_transform_name)\n",
    "\t\t# \t\tnew_w_path = save_path_w + '_' + single_transform_name + '.npy'\n",
    "\t\t# \t\tprint('Before w: ', self.w_embed[single_transform_name][0, :5, 0])\n",
    "\t\t# \t\tself.walk.w_embed[single_transform_name] = torch.Tensor(np.load(new_w_path))\n",
    "\t\t# \t\tprint('After w: ', self.w_embed[single_transform_name][0, :5, 0])\n",
    "\t\t# \t\treturn\n",
    "\t\t#\n",
    "\t\t# \tprint('Load W for embedding')\n",
    "\t\t# \tfor name in self.w_embed.keys():\n",
    "\t\t# \t\tnew_w_path = save_path_w + '_' + name + '.npy'\n",
    "\t\t# \t\tprint('Load W of %s' % name)\n",
    "\t\t# \t\tprint('Before w: ', self.w_embed[name][0, :5, 0])\n",
    "\t\t# \t\tself.walk.w_embed[name] = torch.Tensor(np.load(new_w_path))\n",
    "\t\t# \t\tprint('After w: ', self.w_embed[name][0, :5, 0])\n",
    "\t\t# else:\n",
    "\t\t# \tprint('Load continuous Ws ')\n",
    "\t\t# \tfor name in self.ws.keys():\n",
    "\t\t# \t\tnew_w_path = save_path_w + '_' + name + '.npy'\n",
    "\t\t# \t\tprint('Load W of %s' % name)\n",
    "\t\t# \t\tprint('Before w: ', self.ws[name][0, :5, 0])\n",
    "\t\t# \t\tself.ws[name] = torch.Tensor(np.load(new_w_path))\n",
    "\t\t# \t\tprint('After w: ', self.ws[name][0, :5, 0])\n",
    "\tdef load_model(self, save_path_w, save_path_gan):\n",
    "\t\t# Load w\n",
    "\t\tprint('Load W in %s' % save_path_w)\n",
    "\t\tprint('Before w: ', self.w[0, :5, 0])\n",
    "\t\tself.w = torch.Tensor(np.load(save_path_w))\n",
    "\t\tprint('After w: ', self.w[0, :5, 0])\n",
    "\t\t# Load GAN\n",
    "\t\tprint('Load GAN in %s' % save_path_gan)\n",
    "\t\t# self.module.load(save_path_gan)\n",
    "\t\tself.module = torch.load(save_path_gan)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\tdef save_model(self, save_path_w, save_path_gan):\n",
    "\t\tprint('Save W and GAN in %s and %s' % (save_path_w, save_path_gan))\n",
    "\t\ttorch.save(self.module, save_path_gan)\n",
    "\t\t# self.module.save(save_path_gan)\n",
    "\t\tnp.save(save_path_w, self.w.detach().cpu().numpy())\n",
    "\n",
    "\tdef save_multi_models(self, save_path_w, save_path_gan, trainEmbed=False, updateGAN=True,\n",
    "\t\t\t\t\t\t  single_transform_name=None):\n",
    "\t\tprint('Save W and GAN in %s and %s' % (save_path_w, save_path_gan))\n",
    "\t\tif updateGAN == True:\n",
    "\t\t\tprint('Save GAN')\n",
    "\t\t\ttorch.save(self.module, save_path_gan)\n",
    "\t\t# self.module.save(save_path_gan)\n",
    "\n",
    "\t\ttorch.save(self.walk, save_path_w + '_walk_module.ckpt')\n",
    "\n",
    "\t\tif trainEmbed:\n",
    "\t\t\tif single_transform_name:\n",
    "\t\t\t\tprint('Save %s only ' % single_transform_name)\n",
    "\t\t\t\tcur_path_w = save_path_w + '_' + single_transform_name\n",
    "\t\t\t\tnp.save(cur_path_w, self.walk.w_embe[single_transform_name].detach().cpu().numpy())\n",
    "\t\t\t\treturn\n",
    "\t\t\tprint('Save embed W')\n",
    "\t\t\tfor i, cur_w in self.walk.w_embed.items():\n",
    "\t\t\t\tcur_path_w = save_path_w + '_' + i\n",
    "\t\t\t\tnp.save(cur_path_w, cur_w.detach().cpu().numpy())\n",
    "\t\t\treturn\n",
    "\t\telse:\n",
    "\t\t\tprint('Save ws')\n",
    "\t\t\tfor i, cur_w in self.walk.w_embed.items():\n",
    "\t\t\t\tcur_path_w = save_path_w + '_' + i\n",
    "\t\t\t\tnp.save(cur_path_w, cur_w.detach().cpu().numpy())\n",
    "\n",
    "\"\"\"\n",
    "'''\n",
    "def optimizeParameters(self, feed_dict, name=None, updateGAN=True, trainEmbed=False):\n",
    "\t\"\"\"\n",
    "\tUsed to update single operation. Next one is compatable (better)\n",
    "\t\"\"\"\n",
    "\t#target = feed_dict['target']\n",
    "\tmask = feed_dict['mask_out']\n",
    "\tlogit = feed_dict['logit']\n",
    "\tx_real = feed_dict['real_target']\n",
    "\t# Train discreted W only\n",
    "\tif updateGAN == False and trainEmbed == True:\n",
    "\t\tself.optimizers_embed[name].zero_grad()\n",
    "\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\tself.optimizers_embed[name].step()\n",
    "\t\treturn Edit_loss\n",
    "\n",
    "\telif updateGAN == False and trainEmbed == False:\n",
    "\t\traise('ERROR')\n",
    "\n",
    "\t## ------- updateGAN == True, (ignore trainEmbed) ------- ##\n",
    "\t# Update D\n",
    "\tfor iter_d in range(self.CRITIC_ITERS):\n",
    "\t\tself.module.optimizerD.zero_grad()\n",
    "\t\tlogit = self.get_logits(feed_dict, reshape=False)\n",
    "\t\t#  train with real\n",
    "\t\t# x_resized = x_real.view(-1, self.OUTPUT_DIM)\n",
    "\t\tD_real = self.module.netD(x_real)\n",
    "\t\tD_real = D_real.mean()\n",
    "\t\tD_real.backward(self.module.mone, retain_graph=True)\n",
    "\n",
    "\t\tD_fake = self.module.netD(logit)\n",
    "\t\tD_fake = D_fake.mean()\n",
    "\t\tD_fake.backward(self.module.one, retain_graph=True)\n",
    "\n",
    "\t\t# train with gradient penalty\n",
    "\t\tgp = gradient_penalty(functools.partial(self.module.netD), x_real, logit,\n",
    "\t\t\t\t\t\t\t\t\t\t\tgp_mode='1-gp',\n",
    "\t\t\t\t\t\t\t\t\t\t\tsample_mode='line')\n",
    "\n",
    "\t\t# gradient_penalty = calc_gradient_penalty(self.module.netD, x_real.data, logit.data, self.BATCH_SIZE)\n",
    "\t\tgp.backward(retain_graph=True)\n",
    "\n",
    "\t\t# Wasserstein_D = D_real - D_fake\n",
    "\t\tself.module.optimizerD.step()\n",
    "\n",
    "\t# Update G\n",
    "\tself.module.optimizerG.zero_grad()\n",
    "\n",
    "\tnew_logit = self.get_logits(feed_dict)\n",
    "\tfeed_dict['logit'] = new_logit\n",
    "\t# G_train_loss = self.module.netD(new_logit.view(-1, self.OUTPUT_DIM)).mean()\n",
    "\tG_train_loss = self.module.netD(new_logit).mean()\n",
    "\tEdit_loss = - self.get_edit_loss(feed_dict)\n",
    "\tG_train_loss += Edit_loss\n",
    "\t# print('Edit_loss1 and G loss: ', Edit_loss, G_train_loss)\n",
    "\n",
    "\tG_train_loss.backward(self.module.mone, retain_graph=True)\n",
    "\tself.module.optimizerG.step()\n",
    "\t# Update w\n",
    "\tif name:\n",
    "\t\tif trainEmbed:\n",
    "\t\t\tself.optimizers_embed[name].zero_grad()\n",
    "\t\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\t\tfeed_dict['logit'] = new_logit\n",
    "\t\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t\t# print('Edit_loss2: ', Edit_loss)\n",
    "\t\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\t\tself.optimizers_embed[name].step()\n",
    "\t\telse:\n",
    "\t\t\tself.optimizers[name].zero_grad()\n",
    "\t\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\t\tfeed_dict['logit'] = new_logit\n",
    "\t\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t\t# print('Edit_loss2: ', Edit_loss)\n",
    "\t\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\t\tself.optimizers[name].step()\n",
    "\n",
    "\telse:\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\tfeed_dict['logit'] = new_logit\n",
    "\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t# print('Edit_loss2: ', Edit_loss)\n",
    "\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\tself.optimizer.step()\n",
    "\n",
    "\treturn Edit_loss\n",
    "\n",
    "def optimizeParametersAll(self, feed_dict):\n",
    "\n",
    "\tx_real = feed_dict['real_target']\n",
    "\n",
    "\t# Update D\n",
    "\tfor iter_d in range(self.CRITIC_ITERS):\n",
    "\t\tself.module.optimizerD.zero_grad()\n",
    "\t\tlogit = self.get_logits(feed_dict, reshape=False)\n",
    "\t\t#  train with real\n",
    "\t\t# x_resized = x_real.view(-1, self.OUTPUT_DIM)\n",
    "\t\t# D_real = self.module.netD(x_resized)\n",
    "\t\tD_real = self.module.netD(x_real)\n",
    "\t\tD_real = D_real.mean()\n",
    "\t\tD_real.backward(self.module.mone, retain_graph=True)\n",
    "\n",
    "\t\tD_fake = self.module.netD(logit)\n",
    "\t\tD_fake = D_fake.mean()\n",
    "\t\tD_fake.backward(self.module.one, retain_graph=True)\n",
    "\n",
    "\t\t# train with gradient penalty\n",
    "\t\t# gradient_penalty = calc_gradient_penalty(self.module.netD, x_resized.data, logit.data, self.BATCH_SIZE)\n",
    "\t\tgp = gradient_penalty(functools.partial(self.module.netD), x_real, logit,\n",
    "\t\t\t\t\t\t\t\t\t\t\tgp_mode='1-gp',\n",
    "\t\t\t\t\t\t\t\t\t\t\tsample_mode='line')\n",
    "\n",
    "\t\tgp.backward(retain_graph=True)\n",
    "\t\t# Wasserstein_D = D_real - D_fake\n",
    "\t\tself.module.optimizerD.step()\n",
    "\n",
    "\t# Update G\n",
    "\tself.module.optimizerG.zero_grad()\n",
    "\tnew_logit = self.get_logits(feed_dict)\n",
    "\tfeed_dict['logit'] = new_logit\n",
    "\t# G_train_loss = self.module.netD(new_logit.view(-1, self.OUTPUT_DIM)).mean()\n",
    "\n",
    "\tG_train_loss = self.module.netD(new_logit).mean()\n",
    "\tEdit_loss = - self.get_edit_loss(feed_dict)\n",
    "\tG_train_loss += Edit_loss\n",
    "\tG_train_loss.backward(self.module.mone, retain_graph=True)\n",
    "\tself.module.optimizerG.step()\n",
    "\n",
    "\t# Update w\n",
    "\tfor name in self.optimizers.keys():\n",
    "\t\tself.optimizers[name].zero_grad()\n",
    "\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\tfeed_dict['logit'] = new_logit\n",
    "\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t# print('Edit_loss2: ', Edit_loss)\n",
    "\n",
    "\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\tself.optimizers[name].step()\n",
    "\n",
    "\t# else:\n",
    "\t# \tself.optimizer.zero_grad()\n",
    "\t# \tnew_logit = self.get_logits(feed_dict)\n",
    "\t# \tfeed_dict['logit'] = new_logit\n",
    "\t# \tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t# \t# print('Edit_loss2: ', Edit_loss)\n",
    "\t# \tEdit_loss.backward(retain_graph=True)\n",
    "\t# \tself.optimizer.step()\n",
    "\n",
    "\treturn Edit_loss\n",
    "\n",
    "\n",
    "def optimizeParametersAll(self, feed_dict, trainEmbed, updateGAN):\n",
    "\tif updateGAN:\n",
    "\t\tprint('Update GAN')\n",
    "\t\t# target = feed_dict['target']\n",
    "\t\tmask = feed_dict['mask_out']\n",
    "\t\tlogit = feed_dict['logit']\n",
    "\t\tx_real = feed_dict['real_target']\n",
    "\n",
    "\t\ty_real = Variable(torch.ones(logit.size()[0]).cuda())\n",
    "\t\ty_fake = Variable(torch.zeros(logit.size()[0]).cuda())\n",
    "\n",
    "\t\t# Update D\n",
    "\t\tself.module.optimizerD.zero_grad()\n",
    "\t\tD_real_result = self.module.netD(x_real).squeeze()\n",
    "\t\t# print(D_real_result)\n",
    "\n",
    "\t\tD_real_loss = self.BCE_loss_logits(D_real_result, y_real)\n",
    "\t\tD_fake_result = self.module.netD(logit).squeeze()\n",
    "\t\tD_fake_loss = self.BCE_loss_logits(D_fake_result, y_fake)\n",
    "\t\tD_train_loss = D_real_loss + D_fake_loss\n",
    "\t\t# print('D_train_loss: ', D_train_loss)\n",
    "\t\tD_train_loss.backward(retain_graph=True)\n",
    "\t\tself.module.optimizerD.step()\n",
    "\n",
    "\t\t# Update G\n",
    "\t\tself.module.optimizerG.zero_grad()\n",
    "\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\tfeed_dict['logit'] = new_logit\n",
    "\n",
    "\t\tD_fake_result = self.module.netD(new_logit).squeeze()\n",
    "\t\tG_train_loss = self.BCE_loss_logits(D_fake_result, y_real)\n",
    "\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\tG_train_loss += self.LAMBDA * Edit_loss\n",
    "\t\t# print('G_train_loss: ', G_train_loss)\n",
    "\t\tG_train_loss.backward(retain_graph=True)\n",
    "\t\tself.module.optimizerG.step()\n",
    "\n",
    "\t# Update w\n",
    "\tif trainEmbed:\n",
    "\t\tfor name in self.optimizers_embed.keys():\n",
    "\t\t\tself.optimizers_embed[name].zero_grad()\n",
    "\t\t\tnew_logit = self.get_logits(feed_dict)\n",
    "\t\t\tfeed_dict['logit'] = new_logit\n",
    "\t\t\tEdit_loss = self.get_edit_loss(feed_dict)\n",
    "\t\t\t# print('Edit_loss2: ', Edit_loss)\n",
    "\n",
    "\t\t\tEdit_loss.backward(retain_graph=True)\n",
    "\t\t\tself.optimizers_embed[name].step()\n",
    "\telse:\n",
    "\t\traise('ERROR')\n",
    "\treturn Edit_loss\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6d719-da9b-4dce-93b4-941273379d7a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0baa9a64-eb64-4466-a5bd-8616e98210ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import graphs\n",
    "import importlib\n",
    "import numpy as np\n",
    "import logging\n",
    "import utils.logging\n",
    "from utils import util, image\n",
    "from options.train_options import TrainOptions\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\"\"\"\n",
    "Usage: \n",
    "# face\n",
    "python train.py --model stylegan_v2_real --transform face \\\n",
    "        --num_samples 20000 --learning_rate 1e-4 --latent w \\\n",
    "        --walk_type linear --loss l2 --gpu 3 --attrList Smiling \\\n",
    "        --attrPath './dataset/attributes_celeba.txt' \\\n",
    "        --models_dir ./models_celeba --overwrite_config \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(graphs, graph_inputs, output_dir, attrList,\n",
    "          layers=None, save_freq=100, trainEmbed=False,\n",
    "          updateGAN=False, opt=None):\n",
    "    # configure logging file\n",
    "    logging_file = os.path.join(output_dir, 'log.txt')\n",
    "    if not os.path.exists(output_dir + '/logs/'):\n",
    "        os.mkdir(output_dir + '/logs/')\n",
    "    writer = SummaryWriter(output_dir + '/logs/')\n",
    "    utils.logging.configure(logging_file, append=False)\n",
    "    n_epoch = 10\n",
    "\n",
    "    batch_size = constants.BATCH_SIZE\n",
    "    num_samples = graph_inputs['z'].shape[0]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        if updateGAN:\n",
    "            raise ('ERROR: jointly training is not implemented yet')\n",
    "        else:\n",
    "            ITERS = (num_samples // batch_size)\n",
    "\n",
    "        graph_inputs = graph_util.graph_input(graphs, num_samples, seed=epoch)\n",
    "        print('Number of the training epochs and iterations: ', n_epoch, ITERS)\n",
    "\n",
    "        for i in range(ITERS):\n",
    "            batch_start = i * batch_size\n",
    "            start_time = time.time()\n",
    "            s = slice(batch_start, min(num_samples, batch_start + batch_size))\n",
    "            graph_inputs_batch = util.batch_input(graph_inputs, s)\n",
    "\n",
    "            zs_batch = graph_inputs_batch['z']\n",
    "            graph_inputs_batch_cuda = {}\n",
    "            graph_inputs_batch_cuda['z'] = torch.Tensor(graph_inputs_batch['z']).cuda()\n",
    "\n",
    "            # multi-attribute transformation\n",
    "            z_global = graph_inputs_batch_cuda['z']\n",
    "\n",
    "            # get w = MLP(z)\n",
    "            w_global = graphs.get_w(z_global)\n",
    "            graph_inputs_batch_cuda['w'] = w_global\n",
    "\n",
    "            # get img = netG(w)\n",
    "            out_zs = graphs.get_logits(graph_inputs_batch_cuda)\n",
    "\n",
    "            # get regression preds alpha = R(I_fake) -> N, C\n",
    "            alpha_org = graphs.get_reg_preds(out_zs)\n",
    "\n",
    "            alphas_reg = []\n",
    "\n",
    "            # alpha_for_graph: N x len(attrList), alpha_for_target: len(attrList) (numpy)\n",
    "            alpha_for_graph, alpha_for_target, index_embed = graphs.get_train_alpha(zs_batch,\n",
    "                                                                       N_attr=len(attrList),\n",
    "                                                                       trainEmbed=trainEmbed)\n",
    "\n",
    "            alphas_reg.append(alpha_for_graph)\n",
    "\n",
    "            if not isinstance(alpha_for_graph, list):\n",
    "                alpha_for_graph = [alpha_for_graph]\n",
    "                alpha_for_target = [alpha_for_target]\n",
    "\n",
    "            for ag, at in zip(alpha_for_graph, alpha_for_target):\n",
    "                ag = torch.tensor(ag).float().cuda()\n",
    "                epsilon = graphs.get_alphas(alpha_org, ag)\n",
    "\n",
    "                # w = w + eT\n",
    "                w_new = graphs.get_w_new_tensor(w_global, epsilon,\n",
    "                                                layers=layers)\n",
    "\n",
    "                transformed_inputs = graph_inputs_batch_cuda\n",
    "                transformed_inputs['w'] = w_new\n",
    "                transformed_output = graphs.get_logits(transformed_inputs)\n",
    "                w_global = w_new\n",
    "\n",
    "                feed_dict = {}\n",
    "                feed_dict['w'] = w_global\n",
    "                feed_dict['org'] = out_zs\n",
    "                feed_dict['logit'] = transformed_output\n",
    "                feed_dict['alpha'] = ag\n",
    "\n",
    "            curr_loss = graphs.optimizeParametersAll(feed_dict,\n",
    "                                                     trainEmbed=trainEmbed,\n",
    "                                                     updateGAN=updateGAN,\n",
    "                                                     no_content_loss=opt.no_content_loss,\n",
    "                                                     no_gan_loss=opt.no_gan_loss\n",
    "                                                     )\n",
    "\n",
    "            curr_loss_item = curr_loss.detach().cpu().item()\n",
    "            writer.add_scalar('Loss/train', curr_loss_item, epoch*ITERS+i)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            logging.info('T, epc, bst, lss, alpha: {}, {}, {}, {}, {}'.format(\n",
    "                elapsed_time, epoch, batch_start, curr_loss, round(at[0], 2)))\n",
    "\n",
    "            if (i % save_freq == 0):\n",
    "                make_samples(out_zs, output_dir, epoch, i * batch_size, batch_size,\n",
    "                             name='org_%.2f' % (round(at[0], 2)))\n",
    "                make_samples(transformed_output, output_dir, epoch, i * batch_size, batch_size,\n",
    "                             name='logit_%.2f' % (round(at[0], 2)))\n",
    "\n",
    "        graphs.save_multi_models('{}/model_w_{}'.format(output_dir, epoch),\n",
    "                                 '{}/model_gan_{}.ckpt'.format(output_dir, epoch),\n",
    "                                 trainEmbed=trainEmbed,\n",
    "                                 updateGAN=updateGAN)\n",
    "\n",
    "    graphs.save_multi_models('{}/model_w_{}_final'.format(output_dir, n_epoch),\n",
    "                             '{}/model_gan_{}_final.ckpt'.format(output_dir, n_epoch),\n",
    "                             trainEmbed=trainEmbed,\n",
    "                             updateGAN=updateGAN)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def make_samples(img_tensor, output_dir, epoch, optim_iter, batch_size, pre_path='results', name='test'):\n",
    "    if img_tensor.is_cuda:\n",
    "        img_tensor = img_tensor.detach().cpu().numpy()\n",
    "    img_tensor = np.uint8(np.clip(((img_tensor + 1) / 2.0) * 255, 0, 255))\n",
    "    if img_tensor.shape[1] == 1 or img_tensor.shape[1] == 3:\n",
    "        img_tensor = np.transpose(img_tensor, [0, 2, 3, 1])\n",
    "    image.save_im(image.imgrid(img_tensor, cols=int(math.sqrt(batch_size))),\n",
    "                  '{}/{}/{}_{}_{}'.format(output_dir, pre_path, epoch, optim_iter, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34e88702-2677-47d0-803c-08c64938beef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Getting semantic transform graphs for stylegan_v2_real model...\n",
      "Load transform_base in get_transform_graphs\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing fused: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m graph_util \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m opt\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.graph_util\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m constants \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m opt\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.constants\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgraphs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_model_using_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m g \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_kwargs)\n\u001b[0;32m     78\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mnum_samples\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\__init__.py:9\u001b[0m, in \u001b[0;36mfind_model_using_name\u001b[1;34m(model, transform)\u001b[0m\n\u001b[0;32m      6\u001b[0m modellib \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(model_filename)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#graphs = modellib.get_transform_graphs(model.split('_')[0])\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m graphs \u001b[38;5;241m=\u001b[39m \u001b[43mmodellib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transform_graphs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m target_transform \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\transform_graph_scene.py:10\u001b[0m, in \u001b[0;36mget_transform_graphs\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad transform_base in get_transform_graphs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m transform_base_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.transform_base\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform_base_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m transform_op_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m model \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.transform_op\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad transform op in get_transform_graphs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m transform_op_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\stylegan_v2_real\\transform_base.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stylegan2 \u001b[38;5;28;01mas\u001b[39;00m stylegan\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m ut\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\stylegan_v2_real\\stylegan2.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnetworks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generator, Discriminator\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mStyleGAN\u001b[39;00m():\n\u001b[0;32m     21\u001b[0m \t\u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr):\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\stylegan_v2_real\\networks.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedLeakyReLU, fused_leaky_relu, upfirdn2d\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPixelNorm\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\stylegan_v2_real\\op\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfused_act\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FusedLeakyReLU, fused_leaky_relu\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mupfirdn2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upfirdn2d\n",
      "File \u001b[1;32m~\\Documents\\Sanika_to_New_Laptop\\Class Materials\\EE782\\Project\\Latent2im-main\\Latent2im-main\\graphs\\stylegan_v2_real\\op\\fused_act.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_extension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m      9\u001b[0m module_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m fused \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused_bias_act.cpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused_bias_act_kernel.cu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFusedLeakyReLUFunctionBackward\u001b[39;00m(Function):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, grad_output, out, negative_slope, scale):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1308\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[0;32m   1217\u001b[0m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   1218\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m          is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1227\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;124;03m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;124;03m        ...     verbose=True)\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m-> 1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:1736\u001b[0m, in \u001b[0;36m_jit_compile\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[1;32m-> 1736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_module_from_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2136\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[1;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[0;32m   2134\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2136\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_from_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[0;32m   2138\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing fused: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "#opt = TrainOptions().parse()\n",
    "class BigGAN():\n",
    "    def __init__(self):\n",
    "        self.category = None\n",
    "class StyleGAN():\n",
    "    def __init__(self):\n",
    "        self.latent = 'w'\n",
    "class Color():\n",
    "    def __init__(self):\n",
    "        self.channel = None\n",
    "\n",
    "class TrainingOptions():\n",
    "    def __init__(self):\n",
    "        self.output_dir = None\n",
    "        self.model = 'stylegan_v2_real'\n",
    "        self.transform = 'face'\n",
    "        self.num_samples = 20000\n",
    "        self.learning_rate = 10**(-4)\n",
    "        self.latent = 'w'\n",
    "        self.walk_type = 'linear'\n",
    "        self.loss = '12'\n",
    "        self.gpu = '0'\n",
    "        self.attrList = 'Smiling'\n",
    "        self.attrPath = './dataset/attributes_celeba.txt'\n",
    "        self.models_dir = './models_celeba'\n",
    "        self.overwrite_config = True\n",
    "        self.name = None\n",
    "        self.biggan = BigGAN()\n",
    "        self.stylegan = StyleGAN()\n",
    "        self.prefix = None\n",
    "        self.suffix = None\n",
    "        self.trainEmbed = False\n",
    "        self.updateGAN = False\n",
    "        self.color = Color()\n",
    "\n",
    "opt = TrainingOptions()\n",
    " # output directory\n",
    "if opt.name:\n",
    "    output_dir = opt.name\n",
    "else:\n",
    "    output_dir = '_'.join([opt.model, opt.transform, opt.walk_type,'lr'+str(opt.learning_rate), opt.loss])\n",
    "    if opt.model == 'biggan':\n",
    "        subopt = opt.biggan\n",
    "        if subopt.category:\n",
    "            output_dir += '_cat{}'.format(subopt.category)\n",
    "    elif 'stylegan' in opt.model:\n",
    "        subopt = opt.stylegan\n",
    "        output_dir += '_{}'.format(subopt.latent)\n",
    "\n",
    "    if opt.transform.startswith('color') and opt.color.channel is not None:\n",
    "        output_dir += '_chn{}'.format(opt.color.channel)\n",
    "\n",
    "\n",
    "if opt.suffix:\n",
    "    output_dir += opt.suffix\n",
    "if opt.prefix:\n",
    "    output_dir = opt.prefix + output_dir\n",
    "\n",
    "opt.output_dir = os.path.join(opt.models_dir, output_dir)\n",
    "\n",
    "print(opt.gpu)\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = opt.gpu\n",
    "\n",
    "output_dir = opt.output_dir\n",
    "if not os.path.exists(os.path.join(output_dir, 'results')):\n",
    "        os.makedirs(os.path.join(output_dir, 'results'))\n",
    "\n",
    "# set attrTable\n",
    "graph_kwargs = util.set_graph_kwargs(opt)\n",
    "\n",
    "graph_util = importlib.import_module('graphs.' + opt.model + '.graph_util')\n",
    "constants = importlib.import_module('graphs.' + opt.model + '.constants')\n",
    "model = graphs.find_model_using_name(opt.model, opt.transform)\n",
    "\n",
    "g = model(**graph_kwargs)\n",
    "\n",
    "num_samples = opt.num_samples\n",
    "graph_inputs = graph_util.graph_input(g, num_samples, seed=0)\n",
    "\n",
    "if opt.suffix:\n",
    "    name = opt.suffix\n",
    "else:\n",
    "    name = None\n",
    "\n",
    "attrList = graph_kwargs['attrList']\n",
    "layers = opt.layers\n",
    "\n",
    "print('attrlist: ', attrList)\n",
    "\n",
    "train(g, graph_inputs, output_dir,\n",
    "        attrList,\n",
    "        layers=layers,\n",
    "        save_freq=opt.model_save_freq,\n",
    "        trainEmbed=opt.trainEmbed,\n",
    "        updateGAN=opt.updateGAN,\n",
    "        opt=opt\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a13fedc-fe8d-446b-994d-fbcc5d3a2024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
